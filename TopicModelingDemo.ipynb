
        {
            "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["// can\u0027t yet format YamlFrontmatter ([\"title: Predicting Returns with text data\"; \"category: Scripts\"; \"categoryindex: 2\"; \"index: 4\"], Some { StartLine = 2 StartColumn = 0 EndLine = 6 EndColumn = 8 }) to pynb markdown\n",
"\n",
"[![Script](img/badge-script.svg)](/ConferenceCalls//TopicModelingDemo.fsx)\u0026emsp;\n",
"[![Notebook](img/badge-notebook.svg)](/ConferenceCalls//TopicModelingDemo.ipynb)\n",
"\n",
"\n",
"## Import packages and load scripts\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 1, "outputs": [], 
           "source": ["#r \"nuget: Plotly.NET, 2.0.0-preview.6\"\n",
"#r \"nuget: Newtonsoft.Json\"\n",
"#r \"nuget: MathNet.Numerics.FSharp, 4.15.0\"\n",
"\n",
"open System\n",
"\n",
"Environment.CurrentDirectory \u003c- __SOURCE_DIRECTORY__\n",
"fsi.AddPrinter\u003cDateTime\u003e(fun dt -\u003e dt.ToString(\"s\"))\n",
"\n",
"open Newtonsoft.Json\n",
"open Plotly.NET\n",
"open MathNet.Numerics.LinearAlgebra\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["## Read transcripts from json file\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 2, "outputs": [], 
           "source": ["type Label = \n",
"    | Positive\n",
"    | Negative\n",
"    | Neutral\n",
"\n",
"type LabeledTranscript = \n",
"    { TickerExchange: (string * string) \n",
"      EarningsCall: string\n",
"      CumulativeReturn: float \n",
"      Label: Label }\n",
"\n",
"let readJson (jsonFile: string) =\n",
"    IO.File.ReadAllText(jsonFile)\n",
"    |\u003e fun json -\u003e JsonConvert.DeserializeObject\u003carray\u003cLabeledTranscript\u003e\u003e(json)\n",
"\n",
"let train, test = \n",
"    let rnd = System.Random(42)\n",
"    readJson (\"data-cache/LabeledTranscriptsFullSample.json\")\n",
"    |\u003e Seq.sortBy (fun _ -\u003e rnd.Next())\n",
"    |\u003e Seq.toArray\n",
"    |\u003e fun xs -\u003e \n",
"        let cutoff = float xs.Length * 0.8\n",
"        xs.[.. int cutoff], xs.[int cutoff + 1 ..]\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["## Text Preprocessing\n",
"\n",
"Text data, unstructured data ... \n",
"\n",
"#### 1) Normalization\n",
"\n",
"- Change all words in each article to lower case letters\n",
"- Expand contractions such as \"haven\u0027t\" to \"have not\"\n",
"- Delete numbers, punctuation, special symbols, and non-English words\n",
"\n",
"#### 2) Lemmatiazation/Stemming\n",
"\n",
"- Analyze words as a single root, e.g, \"dissapointment\" to \"dissapoint\"\n",
"- Porters algorithm \n",
"\n",
"#### 3) Tokenization\n",
"\n",
"Split each article into a list of words or phrases or nGrams\n",
"\n",
"**Original**: \"The five boxing wizards jump quickly\"\n",
"\n",
"**1Gram**: \n",
"\n",
"- [\"The\", \"five\", \"boxing\", \"wizards\", \"jump\", \"quickly\"]\n",
"\n",
"**2Grams**:\n",
"\n",
"- [\"The five\", \"five boxing\", \"boxing wizards\", \"wizards jump\", \"jump quickly\"]\n",
"\n",
"#### 4) Stop words removal\n",
"\n",
"- Removing stop words such as \"and\", \"the\", \"is\", and \"are\"\n",
"- List of stopwords taken from item 70 on http://www.nltk.org/nltk_data/.\n",
"\n",
"#### 5) Bag of words\n",
"\n",
"- Transform each block of text to a vector of word counts\n",
"\n",
"#### Train and test sets\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 3, "outputs": [], 
           "source": ["#load \"TextPreprocessing.fsx\"\n",
"open Preprocessing.Normalization\n",
"open Preprocessing.Tokenization\n",
"open Preprocessing.NltkData\n",
"\n",
"type CallId = \n",
"    {Ticker: string; Exchange: string} \n",
"\n",
"type WordCount = \n",
"    {Word: string; Count: int}\n",
"\n",
"type Sentiment =\n",
"    | Positive\n",
"    | Negative\n",
"\n",
"type Call = \n",
"    { CallId: CallId\n",
"      WordCount: WordCount []\n",
"      Signal: float } with\n",
"    \n",
"    member this.Flag =\n",
"        if this.Signal \u003e 0. then Positive\n",
"        else Negative\n",
"\n",
"let preprocessText (text: string) = \n",
"    // Normalization\n",
"    text\n",
"    |\u003e getOnlyWords\n",
"    |\u003e expandContractions\n",
"    // Tokenization\n",
"    |\u003e nGrams 1\n",
"    // Stop words removal\n",
"    |\u003e Seq.choose removeStopWords\n",
"\n",
"let generateCall (xs: LabeledTranscript) = \n",
"    let callId = {Ticker = fst xs.TickerExchange ; Exchange = snd xs.TickerExchange}\n",
"    let wordCount = \n",
"        xs.EarningsCall\n",
"        |\u003e preprocessText\n",
"        // Bag of words\n",
"        |\u003e Seq.countBy id\n",
"        |\u003e Seq.map (fun (word, count) -\u003e {Word=word; Count=count})\n",
"        |\u003e Seq.toArray\n",
"\n",
"    { CallId = callId\n",
"      WordCount = wordCount\n",
"      Signal = xs.CumulativeReturn }\n",
"\n",
"let trainCalls, testCalls = \n",
"    train\n",
"    |\u003e Array.Parallel.map generateCall,\n",
"\n",
"    test \n",
"    |\u003e Array.Parallel.map generateCall\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["## Introduction to Topic modeling\n",
"\n",
"Topic detection or **topic modeling** is a technique of automatically extracting meaning from texts\n",
"by identifying recurrent themes or topics.\n",
"\n",
"Topic modeling is a method for analyzing large volumes of unlabeld text data. It helps in:\n",
"- Discovering hidden topical patterns that are present across the collection\n",
"\n",
"- Annotating documents according to these topics\n",
"\n",
"- Using these annotations to organize, search and summarize texts\n",
"\n",
"A *topic* consists of a cluster of words that frequently occur together.\n",
"This is essentially a clustering problem - we can think of both words and documents as being clustered.\n",
"\n",
"There are many techniques that are used to obtain topic models. One of the commonly used is \n",
"**Latent Dirichlet Allocation (LDA)**\n",
"\n",
"## Predicting Returns with Text Data\n",
"\n",
"**SESTM: A Supervised Sentiment Extraction Algorithm**\n",
"\n",
"Methodology:\n",
"\n",
"1. Feature selection: create a set of sentiment-charged words via *predictive (correlation) screening*\n",
"2. Assign prediction/sentiment weights to these words via a supervised topic model (i.e. estimate positive and negative sentiment topics)\n",
"3. Aggregate terms into an article-level predictive score via penalized likelihood.\n",
"\n",
"- Model is motivated by the view that return-predictive content of a given event is \n",
"reflected *both* in the news article text and in the returns of related assets.\n",
"\n",
"- Method has an objective of extracting general *return predictive* content from text.\n",
"\n",
"**Advantages**\n",
"\n",
"- Simplicity: only requires standard econometric techniques such as correlation analysis and maximum likelihood estimation. \n",
"Additionally, unlike other deep learning approaches, the proposed *supervised* learning approach is entirely \"white-box\".\n",
"\n",
"- Minimal computing power required.\n",
"\n",
"- Free of any pre-existing sentiment dictionary (polarized words, sentiment lexicons, etc...). No use of ad hoc word-weighting schemes.\n",
"\n",
"Bottom line: A sentiment scoring model is constructed from the *joint* behaviour of \n",
"article text and stock returns.\n",
"\n",
"**Theoretical Reusults**\n",
"\n",
"- The guarantee of retrieving a sentiment dictionary from training data via correlation screening.\n",
"\n",
"- The derivation of sharp error bounds for parameter estimation. The error bounds depend on the scale\n",
"of the corpus (e.g., size of the vocabulary, total number of text documents, average number of words \n",
"per document, etc.), and the strength of sentiment signals (e.g., the sentivity of returns to sentiment, \n",
"sensitivity of text generation to sentiment, etc.).\n",
"\n",
"- The error of predicting the sentiment score of a newly arriving article is both derived and quantified.\n",
"\n",
"### A Probabilistic Model for Sentiment Analysis\n",
"\n",
"### 1) Screening for Sentiment-Charged words\n",
"\n",
"Objective: Isolate the subset of sentiment-charged words (remove sentiment-neutral words, i.e. noise).\n",
"\n",
"Intuitively, if a word frequently co-occurs in articles that are accompanied\n",
"by positive returns, that word is likely to convey positive sentiment.\n",
"\n",
"Methodology:\n",
"\n",
"1. Calculate the frequency with which each word (or phrase) *j* co-occurs with a positive\n",
"return. (screening-score $f_{j}$)\n",
"\n",
"2. Compare $f_{j}$ with proper thresholds and create the sentiment-charged set of words $S$.\n",
"\n",
"#### 1A) Screening Score\n",
"\n",
"Before computing any scores, we need to first find out the frequency and \"occurence\" of each word or text item in the corpus of documents.\n",
"\n",
"While the frequency of each text item or word is simply its total count across all documents, an item\u0027s occurence is equivalent to the total count of *documents* that include it.\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 4, "outputs": [], 
           "source": ["/// Vector of item counts per Group (Flag) (Bag of words per group)\n",
"let itemOccurenceByGroup, itemFrequencyByGroup = \n",
"    trainCalls\n",
"    |\u003e Seq.groupBy (fun xs -\u003e xs.Flag)\n",
"    |\u003e Seq.map (fun (group, callsOfGroup) -\u003e \n",
"        callsOfGroup \n",
"        |\u003e Seq.collect (fun xs -\u003e xs.WordCount)\n",
"        |\u003e Seq.groupBy (fun xs -\u003e xs.Word)\n",
"        |\u003e Seq.map (fun (wordId, wordCounts) -\u003e\n",
"            wordCounts\n",
"            |\u003e fun xs -\u003e \n",
"                // Occurence (# of articles that word j appears)\n",
"                (wordId, xs |\u003e Seq.length),\n",
"                // Frequency (total count of word j in all articles)\n",
"                (wordId, xs |\u003e Seq.sumBy (fun xs -\u003e xs.Count)))\n",
"        |\u003e Seq.toArray\n",
"        |\u003e fun xs -\u003e\n",
"            (group, xs |\u003e Array.map fst |\u003e Map), \n",
"            (group, xs |\u003e Array.map snd |\u003e Map))\n",
"    |\u003e Seq.toArray\n",
"    |\u003e fun xs -\u003e \n",
"        (xs |\u003e Array.map fst |\u003e Map),\n",
"        (xs |\u003e Array.map snd |\u003e Map)\n",
"\n",
"/// Frequency/Occurence finder\n",
"let countOfItemInGroup (group: Sentiment)\n",
"                       (wordSentimentMap : Map\u003cSentiment, Map\u003cstring, int\u003e\u003e)\n",
"                       (item: string) = \n",
"    wordSentimentMap.TryFind group\n",
"    |\u003e Option.bind (fun xs -\u003e xs.TryFind item)\n",
"\n",
"countOfItemInGroup Positive itemFrequencyByGroup \"sales\"\n",
"countOfItemInGroup Positive itemOccurenceByGroup \"sales\"\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["While the frequency of each text item or word is simply its total count across all documents, an item\u0027s occurence is equivalent to the total count of *documents* that include text item *j*.\n",
"\n",
"We can then define two variants of screening scores:\n",
"\n",
"1. Screening score based on total word frequency:\n",
"\n",
"$$f_{j} = \\frac{{\\text{count of word } j \\text{ in articles with } sgn(y) = +1 }}{\\text{count of word } j \\text{ in all articles}} $$\n",
"\n",
"2. Screening score based on word occurence across documents:\n",
"\n",
"$$f_{j}^{*} = \\frac{{\\text{count of articles including word } j \\text{ in articles with } sgn(y) = +1 }}{\\text{count of articles including word } j }$$\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 5, "outputs": [], 
           "source": ["type CountType = \n",
"    | Frequency\n",
"    | Occurence\n",
"\n",
"type TextItemScreening =\n",
"    { TextItem: string\n",
"      Score: float\n",
"      Count: float \n",
"      CountType: CountType }\n",
"\n",
"/// Vocabulary (training set only)\n",
"let vocabulary =\n",
"    trainCalls\n",
"    |\u003e Seq.collect (fun xs -\u003e xs.WordCount |\u003e Array.map (fun xs -\u003e xs.Word))\n",
"    |\u003e Seq.distinct\n",
"    |\u003e Seq.toArray\n",
"\n",
"/// Get scores from given word sentiment map (Frequency or Occurence)\n",
"let getScores (wordSentimentMap: Map\u003cSentiment, Map\u003cstring, int\u003e\u003e)\n",
"              (countType: CountType) = \n",
"\n",
"    let getItemScore item = \n",
"        let generateItemScore item score count = \n",
"            {TextItem = item; Score = score; Count = count; CountType = countType }\n",
"\n",
"        let posN, negN = \n",
"            countOfItemInGroup Positive wordSentimentMap item,\n",
"            countOfItemInGroup Negative wordSentimentMap item \n",
"\n",
"        match posN, negN with\n",
"        | Some p, Some n -\u003e \n",
"            let count = float (p + n)\n",
"            let score = (float p) / count\n",
"            Some (generateItemScore item score count)\n",
"        | Some p, None -\u003e \n",
"            let score, count = 1., float p\n",
"            Some (generateItemScore item score count)\n",
"        | None, Some n -\u003e \n",
"            let score, count = 0., float n\n",
"            Some (generateItemScore item score count)\n",
"        | _ -\u003e  None\n",
"    \n",
"    vocabulary\n",
"    // Compute text item scores\n",
"    |\u003e Array.Parallel.choose getItemScore\n",
"    |\u003e Array.map (fun xs -\u003e xs.TextItem, xs)\n",
"    |\u003e Map\n",
"\n",
"let itemOccurenceScores, itemFrequencyScores = \n",
"    getScores itemOccurenceByGroup Occurence, \n",
"    getScores itemFrequencyByGroup Frequency\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["Histogram: Item scores\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 6, "outputs": [], 
           "source": ["itemFrequencyScores\n",
"|\u003e Map.toArray\n",
"|\u003e Array.map (fun (word, xs) -\u003e xs.Score)\n",
"|\u003e Array.filter (fun xs -\u003e xs \u003e 0.25 \u0026\u0026 xs \u003c 0.75)\n",
"|\u003e Chart.Histogram\n",
"|\u003e Chart.Show\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["#### 1B) Sentiment-charged set of words\n",
"\n",
"$$\\widehat{S} = \\{j: f_{j} \\geq \\widehat{\\pi} + \\alpha_{+}, \\text{ or } f_{j} \\leq \\widehat{\\pi} - \\alpha_{-} \\} \\cap \\{ j: k_{j} \\geq \\kappa\\}$$\n",
"\n",
"- $f_{j} = \\text{Sentiment-screening score of word } j $\n",
"- $\\widehat{\\pi} = \\text{Fraction of articles tagged with a positive return}$\n",
"- $\\alpha_{+} = \\text{Upper sentiment-score threshold}$\n",
"- $\\alpha_{-} = \\text{Lower sentiment-score threshold}$\n",
"- $k_{j} = \\text{count of word } j \\text{ in all articles}$\n",
"\n",
"The thresholds ($\\alpha{+}, \\alpha{-}, \\kappa$) are *hyper-parameters* that can be tuned via cross-validation.\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 7, "outputs": [], 
           "source": ["/// Sentiment-charged words\n",
"let getChargedItems alphaLower alphaUpper kappaPct = \n",
"\n",
"    // Count of text item in all articles\n",
"    let kappa = kappaPct * float train.Length\n",
"\n",
"    // Upper and lower score thresholds\n",
"    let upperThresh, lowerThresh = \n",
"        trainCalls\n",
"        |\u003e Array.filter (fun xs -\u003e xs.Flag = Positive)\n",
"        |\u003e fun xs -\u003e float xs.Length / float train.Length\n",
"        |\u003e fun pieHat -\u003e (pieHat + alphaUpper), (pieHat - alphaLower)\n",
"    \n",
"    let isCharged item = \n",
"        match itemFrequencyScores.TryFind item, itemOccurenceScores.TryFind item with\n",
"        | Some freqScore, Some occScore -\u003e \n",
"            if ((freqScore.Score \u003e= upperThresh || freqScore.Score \u003c= lowerThresh) \u0026\u0026 (occScore.Count \u003e= kappa))\n",
"            then Some item\n",
"            else None\n",
"        | _ -\u003e None\n",
"\n",
"    vocabulary\n",
"    |\u003e Array.choose isCharged\n",
"\n",
"let alphaLower, alphaUpper, kappa  = (0.0075, 0.0075, 0.5)\n",
"let chargedItems = getChargedItems alphaLower alphaUpper kappa\n",
"\n",
"chargedItems.Length\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["#### Filtering original item counts\n",
"- Filter train and test\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 8, "outputs": [], 
           "source": ["let filterCall (call: Call): Call = \n",
"\n",
"    let textItemsFromCall = \n",
"        call.WordCount\n",
"        |\u003e Array.map (fun xs -\u003e xs.Word, xs)\n",
"        |\u003e Map\n",
"\n",
"    let filteredItemCounts = \n",
"        chargedItems\n",
"        |\u003e Array.map (fun chargedWord -\u003e \n",
"            match textItemsFromCall.TryFind chargedWord with\n",
"            | Some wordCount -\u003e wordCount\n",
"            | None -\u003e {Word=chargedWord; Count = 0})\n",
"        |\u003e Array.sortBy (fun xs -\u003e xs.Word)\n",
"    \n",
"    { CallId = call.CallId\n",
"      WordCount = filteredItemCounts\n",
"      Signal = call.Signal }\n",
"\n",
"let chargedTrain = \n",
"    trainCalls\n",
"    |\u003e Array.Parallel.map filterCall\n",
"    // Remove \"empty document vectors\"\n",
"    |\u003e Array.filter (fun xs -\u003e xs.WordCount |\u003e Array.sumBy (fun xs -\u003e xs.Count) |\u003e fun xs -\u003e xs \u003c\u003e 0)\n",
"\n",
"let getDocumentTermMatrix (calls: Call []) = \n",
"    calls\n",
"    |\u003e Array.sortBy (fun xs -\u003e xs.Signal)\n",
"    |\u003e Array.map (fun xs -\u003e \n",
"        xs.WordCount \n",
"        |\u003e Array.map (fun xs -\u003e double xs.Count))\n",
"    |\u003e matrix\n",
"    |\u003e fun xs -\u003e xs.Transpose()\n",
"\n",
"let chargedDocumentTermMatrix = getDocumentTermMatrix chargedTrain\n",
"\n",
"chargedDocumentTermMatrix.RowCount\n",
"chargedDocumentTermMatrix.ColumnCount\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["## 2. Learning Sentiment Topics\n",
"\n",
"Fitting a two-topic model to the sentiment-charged counts, `chargedItemCountsById`.\n",
"\n",
"Some notation:\n",
"\n",
"$$\\text{Consider a collection of } n \\text{ articles and a dictionary of } m \\text{ words.}$$\n",
"\n",
"$$d_{i} = \\text{word or (phrase) counts of the } i^{th} article$$\n",
"\n",
"$$d_{i, j} = \\text{ number of times word } j \\text{ occurs in article } i$$\n",
"\n",
"$$D = m \\times n \\text{ document term matrix}; D = [d_{1}, ..., d{n}]$$\n",
"\n",
"Model:\n",
"\n",
"$$d_{[S], i} \\sim \\text{Multinomial} (s_{i}, p_{i}O_{+} + (1 - p_{i})O_{-})$$\n",
"\n",
"$$p_{i} = \\text{ article\u0027s sentiment score, } p_{i} \\in [0,1]$$\n",
"\n",
"$$s_{i} = \\text{ total count of sentiment-charged words in article } i$$\n",
"\n",
"$$O_{+} = \\text{ positive sentiment topic}$$\n",
"\n",
"$$O_{-} = \\text{ negative sentiment topic}$$\n",
"\n",
"$$\\mathbb{E}h_{i} = \\mathbb{E}\\frac{d_{[S], i}}{s_{i}} = p_{i}O_{+} + (1 -p_{i})O_{-}$$\n",
"\n",
"Estimate $$H$$ by plugging in $$\\widehat{S}$$ from screening step:\n",
"\n",
"$$\\widehat{h_{i}} = \\frac{d_{[\\widehat{S}], i}}{\\widehat{s}_{i}}$$\n",
"\n",
"$$\\widehat{s}_{i} = \\sum_{j \\in \\widehat{S}}{d_{j, i}}$$\n",
"\n",
"Estimate W using the standardized ranks of returns. For each each article $$i$$ in the training sample $$i = 1, ..., n$$:\n",
"\n",
"$$\\widehat{p}_{i} = \\frac{\\text{rank of } y_{i} \\text{ in } \\{y_{l}\\}_{l=1}^{n}}{n}$$\n",
"\n",
"\n",
"## Estimator $\\widehat{O}$\n",
"\n",
"#### $H$\n",
"\n",
"$$\\widehat{H} = [\\widehat{h_{1}}, \\widehat{h_{2}},..., \\widehat{h_{3}}]$$\n",
"\n",
"$$\\widehat{h_{i}} = \\frac{d_{[\\widehat{S}], i}}{\\widehat{s}_{i}} \\text{      } \\widehat{s}_{i} = \\sum_{j \\in \\widehat{S}}{d_{j, i}}$$\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 9, "outputs": [], 
           "source": ["let bigH = \n",
"    chargedDocumentTermMatrix\n",
"    |\u003e fun m -\u003e\n",
"        m.ToColumnArrays()\n",
"        |\u003e Array.map (fun itemCounts -\u003e \n",
"            let sumOfItemCounts = \n",
"                Array.sum itemCounts \n",
"\n",
"            itemCounts \n",
"            |\u003e Array.map (fun xs -\u003e xs / sumOfItemCounts))\n",
"        |\u003e matrix\n",
"        |\u003e fun xs -\u003e xs.Transpose()\n",
"\n",
"bigH.RowCount\n",
"bigH.ColumnCount\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["#### $W$\n",
"\n",
"$$\\widehat{W} = \\begin{bmatrix} \\widehat{p_{1}} \u0026 \\widehat{p_{2}} \u0026 \\cdots \u0026 \\widehat{p_{n}} \\\\ 1 - \\widehat{p_{1}} \u0026 1 - \\widehat{p_{2}} \u0026 \\cdots \u0026 1 -\\widehat{p_{n}} \\end{bmatrix}$$\n",
"\n",
"$$\\widehat{p}_{i} = \\frac{\\text{rank of } y_{i} \\text{ in } \\{y_{l}\\}_{l=1}^{n}}{n}$$\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 10, "outputs": [], 
           "source": ["let bigW = \n",
"    let n = \n",
"        double chargedDocumentTermMatrix.ColumnCount\n",
"\n",
"    chargedDocumentTermMatrix.ToColumnArrays()\n",
"    |\u003e Array.mapi (fun i _ -\u003e \n",
"        double (i + 1)/ n)\n",
"    |\u003e fun xs -\u003e \n",
"        matrix [|xs; xs |\u003e Array.map (fun p -\u003e (1. - p))|]\n",
"\n",
"bigW.RowCount\n",
"bigW.ColumnCount\n",
"\n",
"bigH.Multiply(bigW.Transpose())\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["#### $O$\n",
"\n",
"$$\\widehat{O} = [\\widehat{h_{1}}, \\widehat{h_{2}},\\ldots, \\widehat{h_{n}}] \\widehat{W}^{\u0027} (\\widehat{W}\\widehat{W}^{\u0027})^{-1}$$\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 11, "outputs": [], 
           "source": ["let bigO = \n",
"    \n",
"    let h, w, w\u0027 = bigH, bigW, bigW.Transpose()\n",
"    let ww\u0027 = w.Multiply(w\u0027)\n",
"\n",
"    h.Multiply(w\u0027).Multiply(ww\u0027.Inverse())\n",
"    |\u003e Matrix.toColArrays\n",
"    |\u003e Array.map (fun col -\u003e \n",
"        col\n",
"        |\u003e Array.map (fun xs -\u003e if xs \u003c 0. then 0. else xs)\n",
"        |\u003e fun onlyPositiveVals -\u003e \n",
"            let norm = Array.sum onlyPositiveVals\n",
"            onlyPositiveVals \n",
"            |\u003e Array.map (fun xs -\u003e xs / norm))\n",
"    |\u003e matrix\n",
"    |\u003e fun m -\u003e m.Transpose()\n",
"\n",
"bigO.RowCount\n",
"bigO.ColumnCount\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["## 3. Scoring New Articles\n",
"\n",
"Estimating $p$ (sentiment score) for new articles using maximum likelihood estimation:\n",
"\n",
"$$\\widehat{p} = \\arg\\max_{p\\in[\\,0, 1]\\,} \\left\\{\\hat{s}^{-1} \\sum_{j \\in \\widehat{S}}{d_{j}\\log \\left(p \\widehat{O}_{+, j} + (\\,1-p)\\,\\widehat{O}_{-, j}\\right) + \\lambda \\log \\left(p\\left(1 - p \\right)\\right) \\right\\}$$\n",
"\n",
"$\\hat{s}\\text{ is the total count of words from } \\widehat{S} \\text{ in the new article,} \\left(d_{j}, \\widehat{O}_{+, j},  \\widehat{O}_{-, j} \\right) \\text{ are the } j \\text{th entries of the corresponding vectors, and } \\lamda $\n",
"\n",
"For sentiment charged words, their corresponding entries in $O$ should be different. Otherwise, these words would not represent any sentiment and should be left out of the set of sentiment charged words. Sentiment neutral words are analogous to useless predictors in a linear model.\n",
"\n",
"Optimization\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 12, "outputs": [], 
           "source": ["let bigOArr = \n",
"    bigO.ToRowArrays()\n",
"\n",
"let objF (call: Call) (p: float) (lambda: float) = \n",
"\n",
"    let filteredCall = filterCall call\n",
"    \n",
"    let sHat = \n",
"        filteredCall\n",
"        |\u003e fun xs -\u003e \n",
"            xs.WordCount \n",
"            |\u003e Array.sumBy (fun xs -\u003e xs.Count) \n",
"            |\u003e fun xs -\u003e (1. / float xs)\n",
"\n",
"    filteredCall.WordCount\n",
"    |\u003e Array.mapi (fun i xs -\u003e \n",
"        let pos = p * bigOArr.[i].[0]\n",
"        let neg = (1. - p) * bigOArr.[i].[1]\n",
"        let d = float xs.Count\n",
"        d * log (pos + neg))\n",
"    |\u003e fun expr -\u003e\n",
"        (sHat *(Array.sum expr))\n",
"        \n",
"let computeScore (call: Call) = \n",
"    [|0. .. 0.01 .. 1.|]\n",
"    |\u003e Array.map (fun scoreP -\u003e (scoreP, call.Signal), (objF call scoreP 0.00001))\n",
"    |\u003e Array.maxBy snd\n",
"\n",
"let testHighSignals = \n",
"    testCalls\n",
"    |\u003e Array.take 100\n",
"    |\u003e Array.map (fun xs -\u003e \n",
"        let res = computeScore xs\n",
"        res)\n",
"    |\u003e Array.sortBy snd\n",
"\n",
"\n",
"let testLowSignals = \n",
"    testCalls\n",
"    |\u003e Array.sortBy (fun xs -\u003e xs.Signal)\n",
"    |\u003e Array.take 500\n",
"    |\u003e Array.averageBy (fun xs -\u003e \n",
"        let res = computeScore xs |\u003e fst\n",
"        res |\u003e fst)\n"]
          }
,
          {
           "cell_type": "markdown",
           "metadata": {},
           
           "source": ["DiffSharp demo\n",
"\n"]
          }
,
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 13, "outputs": [], 
           "source": ["#r \"nuget: DiffSharp-lite, 1.0.0-preview-987646120\"\n",
"\n",
"open DiffSharp\n",
"open DiffSharp.Optim\n",
"\n",
"let computeScore\u0027 (call: Call) (x: Tensor) =\n",
"\n",
"    let filteredCall = filterCall call\n",
"\n",
"    let sHat = \n",
"        filteredCall\n",
"        |\u003e fun xs -\u003e \n",
"            xs.WordCount \n",
"            |\u003e Array.sumBy (fun xs -\u003e xs.Count) \n",
"            |\u003e fun xs -\u003e (1. / float xs)\n",
"\n",
"    let scoreP = x.[0]\n",
"\n",
"    filteredCall.WordCount\n",
"    |\u003e Array.mapi (fun i xs -\u003e \n",
"        let pos = scoreP * bigOArr.[i].[0]\n",
"        let neg = (1. - scoreP) * bigOArr.[i].[1]\n",
"        let d = float xs.Count\n",
"        d * log (pos + neg))\n",
"    |\u003e Array.sum\n",
"    |\u003e fun sumExpr -\u003e \n",
"        (sHat * sumExpr) + (0.001 * log (scoreP * (1. - scoreP)))\n",
"        |\u003e fun xs -\u003e xs * -1.\n",
"    \n",
"let lr, momentum, iters, threshold = 1e-3, 0.5, 1000, 1e-3\n",
"\n",
"let scoreFun = computeScore\u0027 negativeArticleTrain\n",
"\n",
"let scorePGuess = dsharp.tensor([0.5])\n",
"\n",
"let scoreFx, params\u0027 = optim.sgd(scoreFun, scorePGuess, lr=dsharp.tensor(lr), momentum=dsharp.tensor(momentum), nesterov=true, iters=iters, threshold=threshold)\n"]
          }],
            "metadata": {
            "kernelspec": {"display_name": ".NET (F#)", "language": "F#", "name": ".net-fsharp"},
            "langauge_info": {
        "file_extension": ".fs",
        "mimetype": "text/x-fsharp",
        "name": "C#",
        "pygments_lexer": "fsharp",
        "version": "4.5"
        }
        },
            "nbformat": 4,
            "nbformat_minor": 1
        }
        

