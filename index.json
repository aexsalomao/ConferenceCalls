[{"uri":"/ConferenceCalls/EarningsAnnouncementReturn.html","title":"Earnings Announcement Return (EAR)","content":"(**\n---\ntitle: Earnings Announcement Return (EAR)\ncategory: Scripts\ncategoryindex: 2\nindex: 2\n---\n\n[![Script](img/badge-script.svg)]({{root}}/{{fsdocs-source-basename}}.fsx)\u0026emsp;\n[![Notebook](img/badge-notebook.svg)]({{root}}/{{fsdocs-source-basename}}.ipynb)\n*)\n\n(**\n# Earnings Announcement Return (EAR)\n*)\n\n(**\nIn the \u0060TranscriptParsing.fsx\u0060 script, we downloaded earnings call transcripts, \nticker and exchange information, and even the exact date and time of each earnings call. \nOk great, now what ?\n*)\n\n(**\nIn finance, a growing body of literature is concerned with applying \nstate of the art *text-mining* techniques on textual data with the objective \nof conducting *sentiment analysis*. Such analysis is often conducted using \nstatistical learning methods such as *supervised* and *unsupervised* learning. \nThe key difference between these sets of methods/algorithms lies within their purpose. \nWhile supervised learning is used for solving the task of *prediction*, \nunsupervised learning is used for other tasks such as *data inference*. \nAdditionally, as the names suggest, while supervised learning algorithms \n*learn* by working with *labeled datasets*, unsupervised learning \nalgorithms do not. For this very reason, it is often the case that, when compared \nto unsupervised learning, supervised learning techniques are regarded as \nless complex and more \u0022trustworthy\u0022.\n*)\n\n(**\nHere are some examples:\n\n- Supervised learning: Support vector machine, Neural network, Linear and logistics regression, \nrandom forest, and Classification trees.\n\n- Unsupervised learning: K-means, Hierarchical clustering, Principal Component Analysis (PCA)\n*)\n\n(**\nIn the case of the earnings calls dataset that we formed by parsing motley fool, \nwe can *label* each call according to the realized returns around the time of the \nearnings call. We can then use these returns as a proxy that indicates the overall \n\u0022sentiment\u0022 of each call. The literature refers to these returns as the Earnings Announcement \nReturn, or EAR. From EAR, we can proceed to define or label each earnings calls as \nbeing either \u0022Positive\u0022 or \u0022Negative\u0022. The EAR any given firm (stock) is simply its \nabnormal return over a three day window ***centered*** on the earnings announcement.\n*)\n\n(**\n## Import packages and load scripts\n*)\n\nopen System\nEnvironment.CurrentDirectory \u003C- __SOURCE_DIRECTORY__\n\n#load \u0022Types.fsx\u0022\n#load \u0022Common.fsx\u0022\n#r \u0022nuget: FSharp.Data\u0022\n#r \u0022nuget: Plotly.NET, 2.0.0-preview.6\u0022\n\nif false then\n    let tiingoKey = System.Environment.GetEnvironmentVariable \u0022TIINGO_API_KEY\u0022\n    ()\n\nopen Types\nopen Common\nopen Common.Tiingo\n\nopen Newtonsoft.Json\nopen Plotly.NET\n\nfsi.AddPrinter\u003CDateTime\u003E(fun dt -\u003E dt.ToString(\u0022s\u0022))\n\n(**\n### Reading Transcript data from .json file:\n*)\n\n/// JSON data reader\nlet readEarningsCallJson (jsonFile: string) =\n    IO.File.ReadAllText(jsonFile)\n    |\u003E fun json -\u003E JsonConvert.DeserializeObject\u003Carray\u003CEarningsCall\u003E\u003E(json)\n\n/// Calls data\nlet myCalls = \n    [|\n    \u0022data-cache/EarningsCall2018.json\u0022\n    \u0022data-cache/EarningsCall2019.json\u0022\n    \u0022data-cache/EarningsCall2020.json\u0022\n    \u0022data-cache/EarningsCall2021.json\u0022\n    |]\n    |\u003E Array.collect readEarningsCallJson\n    |\u003E Array.sortBy (fun xs -\u003E xs.CallId.Date)\n\n(**\n## Barplots: Timing of calls\n*)\n\nlet callsByTimeOfDay (calls : EarningsCall []) = \n    calls\n    |\u003E Array.countBy (fun xs -\u003E xs.CallId.Date.Hour)\n    |\u003E Array.sortBy (fun (hour, _) -\u003E hour)\n    |\u003E Chart.Column\n    |\u003E Chart.withTitle $\u0022Earnings Calls by time of day (N: {Seq.length calls})\u0022\n    |\u003E Chart.withX_AxisStyle \u0022Hour\u0022\n    |\u003E Chart.withY_AxisStyle \u0022Count\u0022\n\n(*** do-not-eval ***)\nmyCalls |\u003E callsByTimeOfDay |\u003E Chart.Show\n(*** hide ***)\nmyCalls |\u003E callsByTimeOfDay |\u003E GenericChart.toChartHTML\n(*** include-it-raw ***)\n\n(**\n## Tiingo returns\n*)\n\n(**\nProvided that Tiingo supports the ticker we are looking for, we can use \nthe \u0060Tiingo\u0060 module from \u0060Common.fsx\u0060 to download ticker related data such\nas closing price and volume. Since we might be interested in analyzing the \nstock\u0027s movement following the earnings call we\u0027ll fetch data up until 60 days \nafter each call. We can use this same 60 day window of ticker observations to\ncompute the EAR for each call.\n*)\n\n/// Tiingo data\nlet tiingoWindow (tiingoStart: DateTime)\n                 (tiingoEnd: DateTime)\n                 (ticker: string) =\n    let checkObs obs = \n        match obs with\n        | [||] -\u003E None\n        | _ -\u003E Some obs\n        \n    ticker\n    |\u003E Tiingo.request\n    |\u003E Tiingo.startOn tiingoStart\n    |\u003E Tiingo.endOn tiingoEnd\n    |\u003E Tiingo.get\n    |\u003E checkObs\n\nlet calcReturn pv fv = \n    (fv / pv) - 1.0\n\nlet getReturnObs (ticker: string) (obs: TiingoObs []) = \n    obs\n    |\u003E Seq.pairwise\n    |\u003E Seq.map (fun (yesterday, today) -\u003E \n        { Symbol = ticker\n          Date = today.Date\n          Return = calcReturn (float yesterday.AdjClose) (float today.AdjClose) })\n    |\u003E Seq.toArray\n\nlet earBarPlot (ticker: string) = \n    myCalls \n    // Find first matching observation\n    |\u003E Seq.tryFind (fun xs -\u003E xs.CallId.Ticker = ticker) \n    |\u003E Option.bind (fun call -\u003E \n        tiingoWindow (call.CallId.Date.AddDays(-7.)) (call.CallId.Date.AddDays(7.)) ticker\n        |\u003E fun xs -\u003E\n            match xs with\n            | Some obs -\u003E \n                getReturnObs ticker obs\n                // Plot\n                |\u003E Array.map (fun xs -\u003E xs.Date, xs.Return)\n                |\u003E Chart.Bar\n                |\u003E Chart.withTitle \n                    $\u0022{ticker} Earnings Call {call.CallId.Date} Q{call.CallId.FiscalQuarter.ToString()}\u0022\n                |\u003E Some\n            | None -\u003E None)\n\nlet msftEarPlot = earBarPlot \u0022MSFT\u0022\n\n(*** do-not-eval ***)\nmsftEarPlot |\u003E Option.map Chart.Show\n(*** hide ***)\nmsftEarPlot |\u003E Option.map GenericChart.toChartHTML\n(*** include-it-raw ***)\n\n(**\n### Earnings Announcement Return\n*)\n\n/// Sample range\nlet startSample, endSample =    \n    myCalls\n    |\u003E Seq.map (fun xs -\u003E xs.CallId.Date)\n    |\u003E fun dates -\u003E \n        (Seq.min dates).AddDays(-10.), (Seq.max dates).AddDays(10.)\n\n/// SP500 (SPY)\nlet spyObs = \n    let spyTicker = \u0022SPY\u0022\n\n    let getReturnsMap (tiingoObs: Tiingo.TiingoObs []) =\n        getReturnObs spyTicker tiingoObs\n        |\u003E Array.map (fun xs -\u003E xs.Date, xs)\n        |\u003E Map\n\n    let checkSpy rets =\n        match rets with \n        | None -\u003E failwith \u0022why isn\u0027t Tiingo working\u0022\n        | Some rets -\u003E rets\n        \n    spyTicker\n    |\u003E tiingoWindow startSample endSample\n    |\u003E checkSpy\n    |\u003E getReturnsMap\n\n(**\n## Earnings Announcement Return\n*)\n\n(**\n### Tiingo observation window\n*)\n\n(**\nSometimes a call might happen on a friday or right before or after a long holiday. \nIn these particular case scenarios, we have to be extra careful when trying to find \nour three-day return window.\n\nBecause we don\u0027t have a database with all the non-trading days of a given year, \ninstead of trying to match a three-day return window instantaneously, it is safer \nif we work from a range of return observations and try to find our three-day return \nwindow from there. \n*)\n\n(**\n#### Three-day windows\n*)\n\n/// Three day return window\nlet findThreeDays (middleObs: ReturnObs) (rets: ReturnObs []): ReturnObs [] option = \n    rets\n    |\u003E Seq.windowed 3\n    |\u003E Seq.tryFind (fun retWindow -\u003E\n        let middle = retWindow.[1]\n        middle.Date.Date = middleObs.Date.Date)\n\n/// SPY returns window\nlet spyReturnsBetween (begWin: DateTime) (endWin: DateTime) =\n    let rec loop (date: DateTime) rets =\n        if date.Date \u003C= endWin.Date then\n            match Map.tryFind date spyObs with\n            | Some spy -\u003E loop (date.AddDays(1.0)) (spy::rets)\n            | None -\u003E loop (date.AddDays(1.0)) rets\n        else rets\n    loop begWin []\n\n(**\n#### Adjusted returns\n*)\n\n/// Abnormal returns from three day window\nlet computeAdjReturns (stock : ReturnObs []) = \n    let begWin, endWin = \n        stock\n        |\u003E Seq.sortBy (fun xs -\u003E xs.Date)\n        |\u003E Seq.map (fun xs -\u003E xs.Date)\n        |\u003E fun xs -\u003E \n            (xs |\u003E Seq.head), (xs |\u003E Seq.last)\n    \n    let cumRet rets =\n        (1.0, rets)\n        ||\u003E Seq.fold(fun acc ret -\u003E acc*(1.0\u002Bret)) \n\n    let spy = \n        spyReturnsBetween begWin endWin\n        |\u003E Seq.map (fun xs -\u003E xs.Return)\n        |\u003E cumRet\n        \n    let stockRet = \n        stock \n        |\u003E Seq.map(fun x -\u003E x.Return) \n        |\u003E cumRet\n\n    stockRet - spy\n\n(**\n#### EAR\n*)\n\ntype Sentiment = \n    | Positive\n    | Negative\n    | Neutral\n\ntype EarningsAnnouncementReturn =\n    { EarningsCall: EarningsCall\n      TiingoObs: Tiingo.TiingoObs []\n      Sentiment: Sentiment option \n      Ear: float option }\n\n/// Find first observed return\nlet firstReturnAfterCall (call: EarningsCall) (returnObs: ReturnObs []) = \n    let date = call.CallId.Date\n    if date.Hour \u003C 16 then date.Date \n    else date.Date.AddDays(1.0)\n    \n    |\u003E fun dateOfCall -\u003E \n        returnObs\n        |\u003E Seq.tryFind (fun xs -\u003E xs.Date.Date \u003E= dateOfCall.Date)\n\nlet computeEar (call: EarningsCall) (tiingoObs: Tiingo.TiingoObs []) = \n\n    let getAdjReturns middleObs returnObs = \n        match findThreeDays middleObs returnObs with\n        | Some threeDayWindow -\u003E Some (computeAdjReturns threeDayWindow)\n        | None -\u003E None\n    \n    getReturnObs call.CallId.Ticker tiingoObs\n    |\u003E fun retObs -\u003E\n        firstReturnAfterCall call retObs\n        |\u003E Option.bind (fun middleObs -\u003E \n            getAdjReturns middleObs retObs)\n\nlet generateEar (call: EarningsCall) = \n    let tiingoWindow = \n        let flatDate = call.CallId.Date.Date\n        tiingoWindow (flatDate.AddDays(-10.0)) (flatDate.AddDays(70.0)) call.CallId.Ticker\n\n    match tiingoWindow with\n    | Some tiingoObs -\u003E \n        // For now lets set Sentiment to None\n        Some { EarningsCall = call\n               TiingoObs = tiingoObs\n               Sentiment = None\n               Ear = computeEar call tiingoObs}\n    | None -\u003E None\n\nlet tslaCall = \n    myCalls\n    |\u003E Array.tryFind (fun xs -\u003E xs.CallId.Ticker = \u0022TSLA\u0022)\n    |\u003E Option.bind generateEar\n\ntslaCall |\u003E Option.bind (fun xs -\u003E xs.Ear)\n\n(**\n### Async methods\n*)\n\nmodule Async =\n    let ParallelThrottled xs = Async.Parallel(xs, 100)\n\nlet asyncCall (call: EarningsCall) =\n    let rec loop attempt n =\n        async {\n            try\n                return generateEar call\n            with e -\u003E\n                if attempt \u003E 0 then\n                    do! Async.Sleep 2000 // Wait 2 seconds in case we\u0027re throttled.\n                    return! loop (attempt - 1) n\n                else return! failwithf \u0022Failed to request \u0027%s\u0027. Error: %O\u0022 call.CallId.Ticker e }\n    loop 10 call \n\nlet asyncCalls (calls: EarningsCall []) = \n    calls\n    |\u003E Seq.map asyncCall\n    |\u003E Async.ParallelThrottled\n    |\u003E Async.RunSynchronously\n    |\u003E Array.choose id\n\n(***do-not-eval***)\nlet getEarsByYear year = \n    myCalls\n    |\u003E Array.filter (fun xs -\u003E xs.CallId.Date.Year = year)\n    |\u003E asyncCalls\n\nlet ears2018 = getEarsByYear 2018\nlet ears2019 = getEarsByYear 2019\nlet ears2020 = getEarsByYear 2020\nlet ears2021 = getEarsByYear 2021\n\nlet calls2018, calls2019, calls2020, calls2021 = \n    myCalls\n    |\u003E fun xs -\u003E \n        (xs |\u003E Array.filter (fun xs -\u003E xs.CallId.Date.Year = 2018)),\n        (xs |\u003E Array.filter (fun xs -\u003E xs.CallId.Date.Year = 2019)),\n        (xs |\u003E Array.filter (fun xs -\u003E xs.CallId.Date.Year = 2020)),\n        (xs |\u003E Array.filter (fun xs -\u003E xs.CallId.Date.Year = 2021))\n\ncalls2018.Length\nears2018.Length\n\ncalls2019.Length\nears2019.Length\n\ncalls2020.Length\nears2020.Length\n\ncalls2021.Length\nears2021.Length\n\n(**\n### Download and Export to Json\n*)\n\n(***do-not-eval***)\nlet earToJson (fileName: string) (ears: EarningsAnnouncementReturn [])  = \n    JsonConvert.SerializeObject(ears)\n    |\u003E fun json -\u003E IO.File.WriteAllText(fileName, json)\n\nearToJson \u0022data-cache/EarningsAnnouncementReturn2018.json\u0022 ears2018\nearToJson \u0022data-cache/EarningsAnnouncementReturn2019.json\u0022 ears2019\nearToJson \u0022data-cache/EarningsAnnouncementReturn2020.json\u0022 ears2020\nearToJson \u0022data-cache/EarningsAnnouncementReturn2021.json\u0022 ears2021"},{"uri":"/ConferenceCalls/Types.html","title":"Types","content":"#r \u0022nuget: FSharp.Data\u0022\n\n#load \u0022Common.fsx\u0022\n\n/// TranscriptParsing\ntype CallId =\n    {\n        Ticker : string \n        Exchange : string\n        Date : System.DateTime\n        FiscalQuarter : int     \n    }\n\ntype EarningsCall = \n    {\n        CallId : CallId\n        Transcript : string [] \n    }\n\n/// EarningsAnnouncementReturn\ntype Sentiment = \n    | Positive\n    | Negative\n    | Neutral\n\ntype EarningsAnnouncementReturn =\n    {\n        EarningsCall : EarningsCall\n        TiingoObs : Common.Tiingo.TiingoObs []\n        Sentiment : Sentiment option \n        Ear : float option \n    }\n\n/// Multinomial Naive Bayes Classifier\n\n/// Tokenizination\ntype Token = string\ntype Tokenizer = string -\u003E Token []\ntype Document = Token []\ntype Class = \n    | Positive\n    | Negative\n    | Neutral\ntype LabelledDocument = Document * Class\n\n/// Bag of Words \ntype Count = int\ntype BagOfWords = (Token * Count) []\ntype LabelledBagOfWords = BagOfWords * Class\n\n/// Naive Bayes Classifier\ntype NbClassifier = BagOfWords -\u003E Class\ntype Prior = float\ntype Likelihood = float\ntype TokenLikelihoods = Map\u003CToken, Likelihood\u003E\ntype TokenScore = float\ntype DocumentScore = float\n\ntype Accuracy = float"},{"uri":"/ConferenceCalls/BugCheck.html","title":"Bug test\n","content":"(**\n# Bug test\nThis is a literate comment\n*)\n\n(*\nThis is a normal comment\n*)\n\n(**\nThis is a literate comment\n*)\n\n// This is a normal comment\n\n(**\nThis is a literate comment\n*)\n\n(*\nThis is a normal comment\n*)\n\n(**\nThis is a literate comment\n*)\n\nprintfn \u0022this is some code.\u0022\n\n(*\nThis is a normal comment\n*)"},{"uri":"/ConferenceCalls/TextPreprocessing.html","title":"TextPreprocessing","content":"namespace Preprocessing\n\nmodule Normalization = \n\n    open System.Text.RegularExpressions\n\n    // Detect sentence boundaries\n    let splitParagraph (paragraph: string) = \n        paragraph.Replace(\u0022.\u0022, \u0022XXXX\u0022)\n                 .Replace(\u0022?\u0022, \u0022XXXX\u0022)\n                 .Replace(\u0022!\u0022, \u0022XXXX\u0022)\n                 .Split(\u0022XXXX\u0022) \n        |\u003E Array.map (fun xs -\u003E xs.Replace(\u0022XXXX\u0022, \u0022\u0022).Trim())\n        |\u003E Array.filter (fun xs -\u003E xs.Length \u003C\u003E 0)\n\n    /// Check for *only* words (Regex)       \n    let getOnlyWords (text: string): string= \n        let onlyWords = Regex(@\u0022(?\u003C!\\S)[a-zA-Z]\\S*[a-zA-Z](?!\\S)\u0022)\n\n        text.Replace(\u0022,\u0022, \u0022\u0022)\n            .Replace(\u0022;\u0022, \u0022\u0022)\n            .Replace(\u0022:\u0022, \u0022\u0022)\n            .Trim()\n            .ToLowerInvariant()\n        |\u003E onlyWords.Matches\n        |\u003E Seq.cast\u003CMatch\u003E\n        |\u003E Seq.map (fun m -\u003E m.Value)\n        |\u003E String.concat(\u0022 \u0022)\n\n    /// Non-exhaustive map of english contractions\n    let englishContractions: Map\u003Cstring, string\u003E= \n        [\n        (\u0022aren\u0027t\u0022, \u0022are not\u0022)\n        (\u0022can\u0027t\u0022, \u0022cannot\u0022)\n        (\u0022could\u0027ve\u0022, \u0022could have\u0022)\n        (\u0022couldn\u0027t\u0022, \u0022could not\u0022)\n        (\u0022dammit\u0022, \u0022damn it\u0022)\n        (\u0022didn\u0027t\u0022, \u0022did not\u0022)\n        (\u0022doesn\u0027t\u0022, \u0022does not\u0022)\n        (\u0022don\u0027t\u0022, \u0022do not\u0022)\n        (\u0022dunno\u0022, \u0022do not know\u0022)\n        (\u0022everybody\u0027s\u0022, \u0022everybody is\u0022)\n        (\u0022everyone\u0027s\u0022, \u0022everyone is\u0022)\n        (\u0022gimme\u0022, \u0022give me\u0022)\n        (\u0022gonna\u0022, \u0022going to\u0022)\n        (\u0022gotta\u0022, \u0022got to\u0022)\n        (\u0022hadn\u0027t\u0022, \u0022had not\u0022)\n        (\u0022had\u0027ve\u0022, \u0022had have\u0022)\n        (\u0022hasn\u0027t\u0022, \u0022has not\u0022)\n        (\u0022haven\u0027t\u0022, \u0022have not\u0022)\n        (\u0022here\u0027s\u0022, \u0022here is\u0022)\n        (\u0022how\u0027ll\u0022, \u0022how will\u0022)\n        (\u0022how\u0027re\u0022, \u0022how are\u0022)\n        (\u0022i\u0027ll\u0022, \u0022I will\u0022)\n        (\u0022i\u0027m\u0022, \u0022I am\u0022)\n        (\u0022imma\u0022, \u0022I am about to\u0022)\n        (\u0022innit\u0022, \u0022is it not\u0022)\n        (\u0022i\u0027ve\u0022, \u0022I have\u0022)\n        (\u0022isn\u0027t\u0022, \u0022is not\u0022)\n        (\u0022it\u0027d\u0022,  \u0022it would\u0022)\n        (\u0022kinda\u0022, \u0022kind of\u0022)\n        (\u0022let\u0027s\u0022, \u0022let us\u0022)\n        (\u0022ma\u0027am\u0022, \u0022madam\u0022)\n        (\u0022mayn\u0027t\u0022, \u0022may not\u0022)\n        (\u0022may\u0027ve\u0022, \u0022may have\u0022)\n        (\u0022methinks\u0022, \u0022I think\u0022)\n        (\u0022mightn\u0027t\u0022, \u0022might not\u0022)\n        (\u0022might\u0027ve\u0022, \u0022might have\u0022)\n        (\u0022mustn\u0027t\u0022, \u0022must not\u0022)\n        (\u0022mustn\u0027t\u0027ve\u0022, \u0022must not have\u0022)\n        (\u0022must\u0027ve\u0022, \u0022must have\u0022)\n        (\u0022needn\u0027t\u0022, \u0022need not\u0022)\n        (\u0022shan\u0027t\u0022, \u0022shall not\u0022)\n        (\u0022should\u0027ve\u0022, \u0022should have\u0022)\n        (\u0022shouldn\u0027t\u0022, \u0022should not\u0022)\n        (\u0022shouldn\u0027t\u0027ve\u0022, \u0022should not have\u0022)\n        (\u0022that\u0027re\u0022, \u0022that are\u0022)\n        (\u0022there\u0027re\u0022, \u0022there are\u0022)\n        (\u0022these\u0027re\u0022, \u0022these are\u0022)\n        (\u0022these\u0027ve\u0022, \u0022these have\u0022)\n        (\u0022they\u0027ll\u0022, \u0022they will\u0022)\n        (\u0022they\u0027ve\u0022, \u0022they have\u0022)\n        (\u0022they\u0027re\u0022, \u0022they are\u0022)\n        (\u0022those\u0027re\u0022, \u0022those are\u0022)\n        (\u0022those\u0027ve\u0022, \u0022those have\u0022)\n        (\u0022wanna\u0022, \u0022want to\u0022)\n        (\u0022wasn\u0027t\u0022, \u0022was not\u0022)\n        (\u0022we\u0027d\u0027ve\u0022, \u0022we would have\u0022)\n        (\u0022we\u0027ll\u0022, \u0022we will\u0022)\n        (\u0022we\u0027re\u0022, \u0022we are\u0022)\n        (\u0022we\u0027ve\u0022, \u0022we have\u0022)\n        (\u0022weren\u0027t\u0022, \u0022were not\u0022)\n        (\u0022what\u0027d\u0022, \u0022what did\u0022)\n        (\u0022what\u0027ve\u0022, \u0022what have\u0022)\n        (\u0022where\u0027d\u0022, \u0022where did\u0022)\n        (\u0022where\u0027re\u0022, \u0022where are\u0022)\n        (\u0022where\u0027ve\u0022, \u0022where have\u0022)\n        (\u0022which\u0027re\u0022, \u0022which are\u0022)\n        (\u0022which\u0027ve\u0022, \u0022which have\u0022)\n        (\u0022who\u0027d\u0027ve\u0022, \u0022who would have\u0022)\n        (\u0022who\u0027re\u0022, \u0022who are\u0022)\n        (\u0022who\u0027s\u0022, \u0022who has\u0022)\n        (\u0022who\u0027ve\u0022, \u0022who have\u0022)\n        (\u0022why\u0027d\u0022, \u0022why did\u0022)\n        (\u0022why\u0027re\u0022, \u0022why are\u0022)\n        (\u0022won\u0027t\u0022, \u0022will not\u0022)\n        (\u0022would\u0027ve\u0022, \u0022would have\u0022)\n        (\u0022wouldn\u0027t\u0022, \u0022would not\u0022)\n        (\u0022wouldn\u0027t\u0027ve\u0022, \u0022would not have\u0022)\n        (\u0022you\u0027ll\u0022, \u0022you will\u0022)\n        (\u0022you\u0027re\u0022, \u0022you are\u0022)\n        (\u0022you\u0027ve\u0022, \u0022you have\u0022)\n        ] |\u003E Map\n\n    /// Tryfind contraction and expand\n    let expand (word: string): option\u003Cstring\u003E=\n        if word.Contains(\u0022\u0027\u0022) then \n            match englishContractions.TryFind word with\n            | Some expandedWord -\u003E Some expandedWord\n            | None -\u003E None\n        else Some word\n\n    let expandContractions (textItem: string) = \n        textItem.Split(\u0022 \u0022)\n        |\u003E Array.choose expand\n        |\u003E String.concat(\u0022 \u0022)\n\nmodule Tokenization =\n\n    /// NGrams Tokenizer \n    let nGrams (n: int) (text: string) = \n        text.Split(\u0022 \u0022)\n        |\u003E Array.windowed n\n        |\u003E Array.map (String.concat(\u0022 \u0022))\n\nmodule NltkData = \n\n    let stopWords = \n        Set [\n        \u0022i\u0022\n        \u0022me\u0022\n        \u0022my\u0022\n        \u0022myself\u0022\n        \u0022we\u0022\n        \u0022our\u0022\n        \u0022ours\u0022\n        \u0022ourselves\u0022\n        \u0022you\u0022\n        \u0022you\u0027re\u0022\n        \u0022you\u0027ve\u0022\n        \u0022you\u0027ll\u0022\n        \u0022you\u0027d\u0022\n        \u0022your\u0022\n        \u0022yours\u0022\n        \u0022yourself\u0022\n        \u0022yourselves\u0022\n        \u0022he\u0022\n        \u0022him\u0022\n        \u0022his\u0022\n        \u0022himself\u0022\n        \u0022she\u0022\n        \u0022she\u0027s\u0022\n        \u0022her\u0022\n        \u0022hers\u0022\n        \u0022herself\u0022\n        \u0022it\u0022\n        \u0022it\u0027s\u0022\n        \u0022its\u0022\n        \u0022itself\u0022\n        \u0022they\u0022\n        \u0022them\u0022\n        \u0022their\u0022\n        \u0022theirs\u0022\n        \u0022themselves\u0022\n        \u0022what\u0022\n        \u0022which\u0022\n        \u0022who\u0022\n        \u0022whom\u0022\n        \u0022this\u0022\n        \u0022that\u0022\n        \u0022that\u0027ll\u0022\n        \u0022these\u0022\n        \u0022those\u0022\n        \u0022am\u0022\n        \u0022is\u0022\n        \u0022are\u0022\n        \u0022was\u0022\n        \u0022were\u0022\n        \u0022be\u0022\n        \u0022been\u0022\n        \u0022being\u0022\n        \u0022have\u0022\n        \u0022has\u0022\n        \u0022had\u0022\n        \u0022having\u0022\n        \u0022do\u0022\n        \u0022does\u0022\n        \u0022did\u0022\n        \u0022doing\u0022\n        \u0022a\u0022\n        \u0022an\u0022\n        \u0022the\u0022\n        \u0022and\u0022\n        \u0022but\u0022\n        \u0022if\u0022\n        \u0022or\u0022\n        \u0022because\u0022\n        \u0022as\u0022\n        \u0022until\u0022\n        \u0022while\u0022\n        \u0022of\u0022\n        \u0022at\u0022\n        \u0022by\u0022\n        \u0022for\u0022\n        \u0022with\u0022\n        \u0022about\u0022\n        \u0022against\u0022\n        \u0022between\u0022\n        \u0022into\u0022\n        \u0022through\u0022\n        \u0022during\u0022\n        \u0022before\u0022\n        \u0022after\u0022\n        \u0022above\u0022\n        \u0022below\u0022\n        \u0022to\u0022\n        \u0022from\u0022\n        \u0022up\u0022\n        \u0022down\u0022\n        \u0022in\u0022\n        \u0022out\u0022\n        \u0022on\u0022\n        \u0022off\u0022\n        \u0022over\u0022\n        \u0022under\u0022\n        \u0022again\u0022\n        \u0022further\u0022\n        \u0022then\u0022\n        \u0022once\u0022\n        \u0022here\u0022\n        \u0022there\u0022\n        \u0022when\u0022\n        \u0022where\u0022\n        \u0022why\u0022\n        \u0022how\u0022\n        \u0022all\u0022\n        \u0022any\u0022\n        \u0022both\u0022\n        \u0022each\u0022\n        \u0022few\u0022\n        \u0022more\u0022\n        \u0022most\u0022\n        \u0022other\u0022\n        \u0022some\u0022\n        \u0022such\u0022\n        \u0022no\u0022\n        \u0022nor\u0022\n        \u0022not\u0022\n        \u0022only\u0022\n        \u0022own\u0022\n        \u0022same\u0022\n        \u0022so\u0022\n        \u0022than\u0022\n        \u0022too\u0022\n        \u0022very\u0022\n        \u0022can\u0022\n        \u0022will\u0022\n        \u0022just\u0022\n        \u0022don\u0022\n        \u0022don\u0027t\u0022\n        \u0022should\u0022\n        \u0022should\u0027ve\u0022\n        \u0022now\u0022\n        \u0022ain\u0022\n        \u0022aren\u0022\n        \u0022aren\u0027t\u0022\n        \u0022couldn\u0022\n        \u0022couldn\u0027t\u0022\n        \u0022didn\u0022\n        \u0022didn\u0027t\u0022\n        \u0022doesn\u0022\n        \u0022doesn\u0027t\u0022\n        \u0022hadn\u0022\n        \u0022hadn\u0027t\u0022\n        \u0022hasn\u0022\n        \u0022hasn\u0027t\u0022\n        \u0022haven\u0022\n        \u0022haven\u0027t\u0022\n        \u0022isn\u0022\n        \u0022isn\u0027t\u0022\n        \u0022ma\u0022\n        \u0022mightn\u0022\n        \u0022mightn\u0027t\u0022\n        \u0022mustn\u0022\n        \u0022mustn\u0027t\u0022\n        \u0022needn\u0022\n        \u0022needn\u0027t\u0022\n        \u0022shan\u0022\n        \u0022shan\u0027t\u0022\n        \u0022shouldn\u0022\n        \u0022shouldn\u0027t\u0022\n        \u0022wasn\u0022\n        \u0022wasn\u0027t\u0022\n        \u0022weren\u0022\n        \u0022weren\u0027t\u0022\n        \u0022won\u0022\n        \u0022won\u0027t\u0022\n        \u0022wouldn\u0022\n        \u0022wouldn\u0027t\u0022\n        ]\n\n    let removeStopWords (textItem: string) = \n\n        let remaining = \n            textItem.Split(\u0022 \u0022)\n            |\u003E Array.filter (fun word -\u003E not (stopWords.Contains word))\n        if Array.isEmpty remaining then None else Some (remaining |\u003E String.concat(\u0022 \u0022))\n\nmodule TermFrequencies = \n\n    let tf bow = \n        \n        let docTokenCounts = \n            Seq.sumBy snd bow\n\n        bow\n        |\u003E Array.map (fun (token, count) -\u003E \n            let tf = (float count)/(float docTokenCounts)\n            token, tf)\n        |\u003E Array.sortByDescending snd\n     \n    let idf bows= \n\n        let numDocs = Seq.length bows\n\n        bows\n        |\u003E Seq.collect (Seq.map fst)\n        |\u003E Seq.countBy id\n        |\u003E Seq.map (fun (token, numDocsWithToken) -\u003E \n            let idf = (float numDocs) / (float numDocsWithToken)\n            token, log idf)\n        |\u003E Seq.sortByDescending snd\n        |\u003E Seq.toArray\n\n    let tfIdf (idf : Map\u003C\u0027Token, float\u003E) \n              bow = \n        \n        let idfPrior = \n            idf \n            |\u003E Map.toArray \n            |\u003E Array.averageBy snd\n\n        tf bow\n        |\u003E Array.choose (fun (token, tf) -\u003E \n            match idf.TryFind token with\n            // Word appeared in train\n            | Some idf -\u003E \n                let tfIdf = tf * idf\n                Some (token, tfIdf)\n            // Word did not appear in train \n            | None -\u003E \n                let tfIdf = tf * idfPrior\n                Some (\u0022UNK\u0022, tfIdf))"},{"uri":"/ConferenceCalls/optDemos.html","title":"optDemos","content":"#r \u0022nuget: DiffSharp-lite, 1.0.0-preview-987646120\u0022\n\nopen DiffSharp\nopen DiffSharp.Optim\n\nlet f (x: Tensor) = \n\n    let x, y = x.[0], x.[1]\n\n    (3. * x ** (3.0)) \u002B 2.0 * y ** 2.0\n\nlet lr, momentum, iters, threshold = 1e-3, 0.5, 1000, 1e-3\n\nlet guess = dsharp.tensor([1.; 0.01])\n\nlet fx, x = optim.sgd(f, guess, lr=dsharp.tensor(lr), momentum=dsharp.tensor(momentum), nesterov=true, iters=iters, threshold=threshold)\n\n\n#r \u0022nuget: Flips, 2.4.5\u0022\nopen Flips\n"},{"uri":"/ConferenceCalls/EarningsClassification.html","title":"Classifying Earnings Calls with Naive Bayes","content":"(**\n---\ntitle: Classifying Earnings Calls with Naive Bayes\ncategory: Scripts\ncategoryindex: 3\nindex: 3\n---\n\n[![Script](img/badge-script.svg)]({{root}}/{{fsdocs-source-basename}}.fsx)\u0026emsp;\n[![Notebook](img/badge-notebook.svg)]({{root}}/{{fsdocs-source-basename}}.ipynb)\n*)\n\n(**\n# Classifying Earnings Calls with Naive Bayes\n*)\n\n(**\nAfter dowloading earnings transcripts from Motley Fool, we proceeded to compute \nthe Earnings Announcement Return (EAR) of each company\u0027s earnings announcement \nin \u0060EarningsAnnouncementReturn.fsx\u0060. \n\nWe can use the EAR of each call as a *proxy* that is meant to measure the market\u0027s \noverall sentiment towards a given earnings call. While a high EAR would indicate \nthat the overall market\u0027s sentiment was positive, a low EAR would \nindicate precicely the opposite.\n\nThere are many machine learning algorithms to choose from when trying to solve \na binary or multi-classification problem. Due to its simplicity and intuitive framework, \na Naive Bayes classifier is often a good place to start.\n*)\n\n(**\n## Import packages and load scripts\n*)\n\nopen System\nopen System.IO\nopen FSharp.Data\nEnvironment.CurrentDirectory \u003C- __SOURCE_DIRECTORY__\n\n#r \u0022nuget: FSharp.Stats\u0022\n#r \u0022nuget: Newtonsoft.Json\u0022\n#r \u0022nuget: Plotly.NET, 2.0.0-preview.6\u0022\n\n#r \u0022nuget: FSharp.Collections.ParallelSeq, 1.1.4\u0022\n#load \u0022Types.fsx\u0022\n#load \u0022TextPreprocessing.fsx\u0022\n\nopen Newtonsoft.Json\nopen Plotly.NET\nopen FSharp.Stats\nopen FSharp.Collections.ParallelSeq\nopen System.Text.RegularExpressions\n\nopen Types\nopen Preprocessing.Normalization\nopen Preprocessing.Tokenization\nopen Preprocessing.NltkData\n\n(**\n## Read transcripts from json file\n*)\n\nlet readEarJson (jsonFile : string) =\n    IO.File.ReadAllText(jsonFile)\n    |\u003E fun json -\u003E JsonConvert.DeserializeObject\u003Carray\u003CEarningsAnnouncementReturn\u003E\u003E(json)\n\nlet myEars = \n    [\n        \u0022data-cache/EarningsAnnouncementReturn2018.json\u0022\n        \u0022data-cache/EarningsAnnouncementReturn2019.json\u0022\n        \u0022data-cache/EarningsAnnouncementReturn2020.json\u0022\n    ]\n    |\u003E Seq.collect readEarJson\n    |\u003E Seq.choose (fun sample -\u003E \n        match sample.Ear with\n        | Some _ -\u003E Some sample\n        | None -\u003E None)\n    |\u003E Seq.sortBy (fun xs -\u003E xs.EarningsCall.CallId.Date)\n    |\u003E Seq.toArray\n\nmyEars.Length\n(*** include-it ***)\n\n(**\n### Data visualization: Earnings Announcement Returns\n*)\n\nlet earsHist (ears : float array) \n             (thresh: float) = \n\n    let obsToPlot name threshExpr =         \n        ears \n        |\u003E Array.filter threshExpr\n        |\u003E fun filteredEars -\u003E\n            let pct = \n                float filteredEars.Length/ float ears.Length \n                |\u003E fun xs -\u003E Math.Round(xs * 100., 2)\n            filteredEars\n            |\u003E Chart.Histogram\n            |\u003E Chart.withTraceName ($\u0022{name} ({pct}%%)\u0022)\n    [\n        obsToPlot \u0022Negative\u0022 (fun ret -\u003E ret \u003C= -thresh)\n        obsToPlot \u0022Neutral\u0022 (fun ret -\u003E abs ret \u003C thresh)\n        obsToPlot \u0022Positive\u0022 (fun ret -\u003E ret \u003E= thresh)\n    ]\n    |\u003E Chart.Combine\n    |\u003E Chart.withTitle (\u0022Earnings Announcement Returns (EAR)\u0022)\n    |\u003E Chart.withX_AxisStyle (\u0022EAR\u0022)\n    |\u003E Chart.withY_AxisStyle (\u0022Count\u0022)\n    |\u003E Chart.withSize (1000., 500.)\n\nlet earsToPlot = \n    myEars\n    |\u003E Array.choose (fun xs -\u003E xs.Ear)\n    // Remove outliers ...\n    |\u003E Array.filter (fun xs -\u003E abs xs \u003C 0.5)\n\n(*** do-not-eval ***)\n/// earsHist earsToPlot 0.05 |\u003E Chart.Show \n(*** hide ***)\n/// earsHist earsToPlot 0.05 |\u003E GenericChart.toChartHTML\n(*** include-it-raw ***)\n\n(**\n## Generate Dataset\n*)\n\nlet labelEar (earVal : float) thresh : Class = \n    if earVal \u003E= thresh then Positive\n    elif earVal \u003C= -thresh then Negative\n    else Neutral\n\nlet trainRaw, testRaw = \n    myEars\n    |\u003E Array.choose (fun xs -\u003E \n        match xs.Ear with\n        | Some ear -\u003E \n            let document = xs.EarningsCall.Transcript |\u003E String.concat(\u0022 \u0022)\n            let label : Class = labelEar ear 0.05\n            if label \u003C\u003E Neutral then Some (document, label)\n            else None\n        | None -\u003E None)\n    |\u003E fun xs -\u003E \n        let cutoff = float xs.Length * 0.8\n        xs.[.. int cutoff], xs.[int cutoff \u002B 1 ..]\n\ntrainRaw.Length\ntestRaw.Length\n\n(**\n#### AdHoc blacklists : \n1. Identifying proper nouns with NLTK database\n2. Fetching company names using requests\n*)\n\n/// Names\nlet nltkNames = \n    System.IO.File.ReadLines(\u0022data-cache\\NamesNLTK.txt\u0022)\n    |\u003E Seq.map (fun xs -\u003E xs.ToLowerInvariant())\n    |\u003E Set\n\n(**\n## Preprocessing Text\n*)\n\n(**\n#### Tokenize documents\n*)\n\n// Tokenize all documents\nlet tokenizeDocumentWith (nGram : int)\n                         (rawDocument : string) \n                         : Token [] = \n    rawDocument.ToLowerInvariant().Split(\u0022 \u0022)\n    // Blacklist filter\n    |\u003E Array.filter (nltkNames.Contains \u003E\u003E not)\n    |\u003E String.concat(\u0022 \u0022)\n    // Normalize\n    |\u003E getOnlyWords\n    |\u003E expandContractions\n    // Tokenize\n    |\u003E nGrams nGram\n    // Stop words removal\n    |\u003E Array.choose removeStopWords\n    // Empty string removal\n    |\u003E Array.filter (fun xs -\u003E xs.Length \u003C\u003E 0)\n\nlet tokenizeDocuments (tokenizer : Tokenizer)\n                      (labelledRawDocuments : (string * Class) []) \n                      : LabelledDocument [] = \n    labelledRawDocuments\n    |\u003E Array.Parallel.map (fun (doc, label) -\u003E \n        tokenizer doc, label)\n\n(**\n#### Bag of words representation\n*)\n\n// Top Tokens from sample\nlet getTopNTokens (sampleMaxTokens : int)\n                  (labelledDocuments : LabelledDocument []) \n                  : Set\u003CToken\u003E = \n    labelledDocuments\n    |\u003E Array.collect fst\n    |\u003E Array.countBy id\n    |\u003E Array.sortByDescending snd\n    |\u003E Array.truncate sampleMaxTokens\n    |\u003E Array.map fst\n    |\u003E Set\n\n// Generate bag of words using only top tokens\nlet getTopTokenBow (topTokens : Set\u003CToken\u003E)\n                   (document : Document) \n                   : BagOfWords = \n    document\n    |\u003E Array.countBy id\n    |\u003E Array.filter (fun (token, _) -\u003E topTokens.Contains token)\n    |\u003E Array.sortByDescending snd\n\nlet generateTopTokenBows (topTokens : Set\u003CToken\u003E)\n                         (labelledDocuments : LabelledDocument []) \n                         : LabelledBagOfWords [] =\n    labelledDocuments\n    |\u003E Array.Parallel.map (fun (doc, label) -\u003E \n        getTopTokenBow topTokens doc, label)\n\n(**\n#### Preprocess text\n*)\n\ntype TextVectorizer = \n    { NGram : int \n      MaxSampleTokens : int}\n\nlet vectorizeTrainTest (textPreprocessor : TextVectorizer)\n                       (rawDocumentsTrain : (string * Class) []) \n                       (rawDocumentsTest : (string * Class) [])\n                       : LabelledBagOfWords [] * LabelledBagOfWords [] = \n\n    // Tokenize documents (nGrams)\n    let tokenizer = \n        tokenizeDocumentWith textPreprocessor.NGram\n\n    let tokenizedTrain, tokenizedTest = \n        tokenizeDocuments tokenizer rawDocumentsTrain,\n        tokenizeDocuments tokenizer rawDocumentsTest\n   \n    // Generate bag of words using most frequent tokens\n    let topTokens = \n        getTopNTokens textPreprocessor.MaxSampleTokens tokenizedTrain\n    \n    generateTopTokenBows topTokens tokenizedTrain,\n    generateTopTokenBows topTokens tokenizedTest\n\n(**\n## Training the Naive Bayes classifier\n*)\n\n(**\n#### Class Priors\n*)\n\nlet getPriors (labelledBows : LabelledBagOfWords []) \n              : Map\u003CClass, Prior\u003E = \n    \n    let n = labelledBows.Length\n    \n    labelledBows\n    |\u003E Array.groupBy snd\n    |\u003E Array.map (fun (label, labelFreqs) -\u003E \n        let prior =  (float (labelFreqs.Length)/(float n))\n        label, prior)\n    |\u003E Map\n\n(**\n#### Aggregate Token Counts by Class -\u003E Class Bag of Words\n*)\n\nlet getClassBagofWords \n    (labelledBow : LabelledBagOfWords [])\n    : (Class * Token * Count) [] = \n    \n    labelledBow\n    |\u003E Array.groupBy snd\n    |\u003E Array.collect (fun (c, classBagOfWords) -\u003E \n        classBagOfWords\n        |\u003E Array.filter (fun (_, label) -\u003E label=c)\n        |\u003E Array.collect fst\n        |\u003E Array.groupBy fst\n        |\u003E Array.map (fun (token, tokenCounts) -\u003E \n            c, token, Array.sumBy snd tokenCounts))\n\n(**\n#### Token Likelihoods by Class\n*)\n\nlet computeTokenLikilihoods \n    (classBagOfWords : (Class * Token * Count) [])\n    (vocabN : Count) \n    : (Class * Token * Likelihood) [] = \n    \n    classBagOfWords\n    |\u003E Array.groupBy (fun (_, token, _) -\u003E token)\n    |\u003E Array.collect (fun (_, xs) -\u003E \n        /// Compute total token counts within all classes\n        let totalTokenCounts = \n            xs\n            |\u003E Array.sumBy (fun (_, _, counts) -\u003E counts)\n        /// Compute token likelihood for all classes (Laplace corrected)\n        xs\n        |\u003E Array.map (fun (c, token, counts) -\u003E \n            let tokenLikelihood = \n                float (counts \u002B 1) / float (totalTokenCounts \u002B vocabN)\n            (c, token, tokenLikelihood)))\n\nlet getClassLikelihoodsMap (tokenLikelihoods :  (Class * Token * Likelihood) []) \n                           : Map\u003CClass, Map\u003CToken, Likelihood\u003E\u003E = \n    tokenLikelihoods\n    |\u003E Array.groupBy (fun (c, _, _) -\u003E c)\n    |\u003E Array.map (fun (c, xs) -\u003E \n        c, \n        xs\n        |\u003E Array.map (fun (_, token, counts) -\u003E token, counts)\n        |\u003E Map)\n    |\u003E Map\n\nlet getTokenLikelihoods (labelledBows : LabelledBagOfWords [])\n                        : Map\u003CClass, Map\u003CToken, Likelihood\u003E\u003E = \n    \n    let classBagOfWords = \n        getClassBagofWords labelledBows\n\n    let vocabN = \n        classBagOfWords\n        |\u003E Array.distinctBy (fun (_, token, _) -\u003E token)\n        |\u003E Array.length\n\n    computeTokenLikilihoods classBagOfWords vocabN\n    |\u003E getClassLikelihoodsMap\n\n(**\n#### Building the Naive Bayes Classifier\n*)\n\ntype NbClassifierInfo = \n    { Priors : Map\u003CClass, Prior\u003E\n      Likelihoods : Map\u003CClass, Map\u003CToken, Likelihood\u003E\u003E}\n\nlet trainNbClassifier (labelledBows : LabelledBagOfWords []) \n                      : NbClassifierInfo = \n    { Priors = getPriors labelledBows\n      Likelihoods = getTokenLikelihoods labelledBows}\n\n(**\n## Classifying new Documents\n*)\n\n/// Fetch token scores from bag of words\nlet getTokenScores \n    (tokenLikelihoods : Map\u003CToken, Likelihood\u003E)\n    (bow : BagOfWords) \n    : TokenScore [] = \n    \n    bow\n    |\u003E Array.choose (fun (token, count) -\u003E \n        match tokenLikelihoods.TryFind token with\n        | Some likelihood -\u003E \n            Some (log (likelihood ** float count))\n        | None -\u003E None)\n\n/// Computes final score by adding token scores, prior\nlet computeDocumentScore \n    (prior : Prior)\n    (tokenScores : TokenScore []) \n    : DocumentScore =\n    \n    tokenScores\n    |\u003E Array.fold (\u002B) (log prior)\n\n/// Computes document scores and classifies document\nlet classifyBagOfWords \n    (nbClassifierInfo : NbClassifierInfo)\n    (bow : BagOfWords)\n    : Class =\n    \n    nbClassifierInfo.Priors\n    |\u003E Map.toArray\n    |\u003E Array.choose (fun (c, prior) -\u003E \n        match nbClassifierInfo.Likelihoods.TryFind c with\n        | Some tokenLikelihoods -\u003E\n            bow\n            |\u003E getTokenScores tokenLikelihoods  \n            |\u003E computeDocumentScore prior\n            |\u003E fun docScore -\u003E Some (c, docScore)\n        | None -\u003E None)   \n    |\u003E Array.maxBy snd\n    |\u003E fst\n\n(**\n### Evaluate\n*)\n\nlet evaluate (nbClassifierInfo : NbClassifierInfo)\n             (labelledBows : LabelledBagOfWords [])\n             : Accuracy =\n    \n    let classifyBow = classifyBagOfWords nbClassifierInfo\n   \n    labelledBows\n    |\u003E PSeq.averageBy (fun (bow, label) -\u003E  \n        if classifyBow bow = label then 1. \n        else 0.)\n\n(**\n### Model 1\n*)\n\nlet tp1 = {NGram=2; MaxSampleTokens=5000}\nlet trainBow, testBow = vectorizeTrainTest tp1 trainRaw testRaw\n\nlet fittedClassifier = trainNbClassifier trainBow\n\nlet trainEval = evaluate fittedClassifier trainBow\nlet testEval = evaluate fittedClassifier testBow"},{"uri":"/ConferenceCalls/TopicModelingDemo.html","title":"Predicting Returns with text data","content":"(**\n---\ntitle: Predicting Returns with text data\ncategory: Scripts\ncategoryindex: 2\nindex: 4\n---\n\n[![Script](img/badge-script.svg)]({{root}}/{{fsdocs-source-basename}}.fsx)\u0026emsp;\n[![Notebook](img/badge-notebook.svg)]({{root}}/{{fsdocs-source-basename}}.ipynb)\n\n*)\n\n(**\n## Import packages and load scripts\n*)\n\n#r \u0022nuget: Plotly.NET, 2.0.0-preview.6\u0022\n#r \u0022nuget: Newtonsoft.Json\u0022\n#r \u0022nuget: MathNet.Numerics.FSharp, 4.15.0\u0022\n\nopen System\n\nEnvironment.CurrentDirectory \u003C- __SOURCE_DIRECTORY__\nfsi.AddPrinter\u003CDateTime\u003E(fun dt -\u003E dt.ToString(\u0022s\u0022))\n\nopen Newtonsoft.Json\nopen Plotly.NET\nopen MathNet.Numerics.LinearAlgebra\n\n(**\n## Read transcripts from json file\n*)\n\ntype Label = \n    | Positive\n    | Negative\n    | Neutral\n\ntype LabeledTranscript = \n    { TickerExchange: (string * string) \n      EarningsCall: string\n      CumulativeReturn: float \n      Label: Label }\n\nlet readJson (jsonFile: string) =\n    IO.File.ReadAllText(jsonFile)\n    |\u003E fun json -\u003E JsonConvert.DeserializeObject\u003Carray\u003CLabeledTranscript\u003E\u003E(json)\n\nlet train, test = \n    let rnd = System.Random(42)\n    readJson (\u0022data-cache/LabeledTranscriptsFullSample.json\u0022)\n    |\u003E Seq.sortBy (fun _ -\u003E rnd.Next())\n    |\u003E Seq.toArray\n    |\u003E fun xs -\u003E \n        let cutoff = float xs.Length * 0.8\n        xs.[.. int cutoff], xs.[int cutoff \u002B 1 ..]\n\n(**\n## Text Preprocessing\n*)\n\n(**\nText data, unstructured data ... \n*)\n\n(**\n#### 1) Normalization\n*)\n\n(**\n- Change all words in each article to lower case letters\n- Expand contractions such as \u0022haven\u0027t\u0022 to \u0022have not\u0022\n- Delete numbers, punctuation, special symbols, and non-English words\n*)\n\n(**\n#### 2) Lemmatiazation/Stemming\n*)\n\n(**\n- Analyze words as a single root, e.g, \u0022dissapointment\u0022 to \u0022dissapoint\u0022\n- Porters algorithm \n*)\n\n(**\n#### 3) Tokenization\n*)\n\n(**\nSplit each article into a list of words or phrases or nGrams\n\n**Original**: \u0022The five boxing wizards jump quickly\u0022\n\n**1Gram**: \n\n- [\u0022The\u0022, \u0022five\u0022, \u0022boxing\u0022, \u0022wizards\u0022, \u0022jump\u0022, \u0022quickly\u0022]\n\n**2Grams**:\n\n- [\u0022The five\u0022, \u0022five boxing\u0022, \u0022boxing wizards\u0022, \u0022wizards jump\u0022, \u0022jump quickly\u0022]\n*)\n\n(**\n#### 4) Stop words removal\n*)\n\n(**\n- Removing stop words such as \u0022and\u0022, \u0022the\u0022, \u0022is\u0022, and \u0022are\u0022\n- List of stopwords taken from item 70 on http://www.nltk.org/nltk_data/.\n*)\n\n(**\n#### 5) Bag of words\n*)\n\n(**\n- Transform each block of text to a vector of word counts\n*)\n\n(**\n#### Train and test sets\n*)\n\n#load \u0022TextPreprocessing.fsx\u0022\nopen Preprocessing.Normalization\nopen Preprocessing.Tokenization\nopen Preprocessing.NltkData\n\ntype CallId = \n    {Ticker: string; Exchange: string} \n\ntype WordCount = \n    {Word: string; Count: int}\n\ntype Sentiment =\n    | Positive\n    | Negative\n\ntype Call = \n    { CallId: CallId\n      WordCount: WordCount []\n      Signal: float } with\n    \n    member this.Flag =\n        if this.Signal \u003E 0. then Positive\n        else Negative\n\nlet preprocessText (text: string) = \n    // Normalization\n    text\n    |\u003E getOnlyWords\n    |\u003E expandContractions\n    // Tokenization\n    |\u003E nGrams 1\n    // Stop words removal\n    |\u003E Seq.choose removeStopWords\n\nlet generateCall (xs: LabeledTranscript) = \n    let callId = {Ticker = fst xs.TickerExchange ; Exchange = snd xs.TickerExchange}\n    let wordCount = \n        xs.EarningsCall\n        |\u003E preprocessText\n        // Bag of words\n        |\u003E Seq.countBy id\n        |\u003E Seq.map (fun (word, count) -\u003E {Word=word; Count=count})\n        |\u003E Seq.toArray\n\n    { CallId = callId\n      WordCount = wordCount\n      Signal = xs.CumulativeReturn }\n\nlet trainCalls, testCalls = \n    train\n    |\u003E Array.Parallel.map generateCall,\n\n    test \n    |\u003E Array.Parallel.map generateCall\n\n(**\n## Introduction to Topic modeling\n*)\n\n(**\nTopic detection or **topic modeling** is a technique of automatically extracting meaning from texts\nby identifying recurrent themes or topics.\n\nTopic modeling is a method for analyzing large volumes of unlabeld text data. It helps in:\n- Discovering hidden topical patterns that are present across the collection\n\n- Annotating documents according to these topics\n\n- Using these annotations to organize, search and summarize texts\n\nA *topic* consists of a cluster of words that frequently occur together.\nThis is essentially a clustering problem - we can think of both words and documents as being clustered.\n\nThere are many techniques that are used to obtain topic models. One of the commonly used is \n**Latent Dirichlet Allocation (LDA)**\n*)\n\n(**\n## Predicting Returns with Text Data\n*)\n\n(**\n**SESTM: A Supervised Sentiment Extraction Algorithm**\n\nMethodology:\n\n1. Feature selection: create a set of sentiment-charged words via *predictive (correlation) screening*\n2. Assign prediction/sentiment weights to these words via a supervised topic model (i.e. estimate positive and negative sentiment topics)\n3. Aggregate terms into an article-level predictive score via penalized likelihood.\n\n- Model is motivated by the view that return-predictive content of a given event is \nreflected *both* in the news article text and in the returns of related assets.\n\n- Method has an objective of extracting general *return predictive* content from text.\n*)\n\n(**\n**Advantages**\n\n- Simplicity: only requires standard econometric techniques such as correlation analysis and maximum likelihood estimation. \nAdditionally, unlike other deep learning approaches, the proposed *supervised* learning approach is entirely \u0022white-box\u0022.\n\n- Minimal computing power required.\n\n- Free of any pre-existing sentiment dictionary (polarized words, sentiment lexicons, etc...). No use of ad hoc word-weighting schemes.\n\nBottom line: A sentiment scoring model is constructed from the *joint* behaviour of \narticle text and stock returns.\n*)\n\n(**\n**Theoretical Reusults**\n\n- The guarantee of retrieving a sentiment dictionary from training data via correlation screening.\n\n- The derivation of sharp error bounds for parameter estimation. The error bounds depend on the scale\nof the corpus (e.g., size of the vocabulary, total number of text documents, average number of words \nper document, etc.), and the strength of sentiment signals (e.g., the sentivity of returns to sentiment, \nsensitivity of text generation to sentiment, etc.).\n\n- The error of predicting the sentiment score of a newly arriving article is both derived and quantified.\n*)\n\n(**\n### A Probabilistic Model for Sentiment Analysis\n*)\n\n(**\n### 1) Screening for Sentiment-Charged words\n*)\n\n(**\nObjective: Isolate the subset of sentiment-charged words (remove sentiment-neutral words, i.e. noise).\n\nIntuitively, if a word frequently co-occurs in articles that are accompanied\nby positive returns, that word is likely to convey positive sentiment.\n\nMethodology:\n\n1. Calculate the frequency with which each word (or phrase) *j* co-occurs with a positive\nreturn. (screening-score $f_{j}$)\n\n2. Compare $f_{j}$ with proper thresholds and create the sentiment-charged set of words $S$.\n*)\n\n(**\n#### 1A) Screening Score\n*)\n\n(**\nBefore computing any scores, we need to first find out the frequency and \u0022occurence\u0022 of each word or text item in the corpus of documents.\n\nWhile the frequency of each text item or word is simply its total count across all documents, an item\u0027s occurence is equivalent to the total count of *documents* that include it.\n*)\n\n/// Vector of item counts per Group (Flag) (Bag of words per group)\nlet itemOccurenceByGroup, itemFrequencyByGroup = \n    trainCalls\n    |\u003E Seq.groupBy (fun xs -\u003E xs.Flag)\n    |\u003E Seq.map (fun (group, callsOfGroup) -\u003E \n        callsOfGroup \n        |\u003E Seq.collect (fun xs -\u003E xs.WordCount)\n        |\u003E Seq.groupBy (fun xs -\u003E xs.Word)\n        |\u003E Seq.map (fun (wordId, wordCounts) -\u003E\n            wordCounts\n            |\u003E fun xs -\u003E \n                // Occurence (# of articles that word j appears)\n                (wordId, xs |\u003E Seq.length),\n                // Frequency (total count of word j in all articles)\n                (wordId, xs |\u003E Seq.sumBy (fun xs -\u003E xs.Count)))\n        |\u003E Seq.toArray\n        |\u003E fun xs -\u003E\n            (group, xs |\u003E Array.map fst |\u003E Map), \n            (group, xs |\u003E Array.map snd |\u003E Map))\n    |\u003E Seq.toArray\n    |\u003E fun xs -\u003E \n        (xs |\u003E Array.map fst |\u003E Map),\n        (xs |\u003E Array.map snd |\u003E Map)\n\n/// Frequency/Occurence finder\nlet countOfItemInGroup (group: Sentiment)\n                       (wordSentimentMap : Map\u003CSentiment, Map\u003Cstring, int\u003E\u003E)\n                       (item: string) = \n    wordSentimentMap.TryFind group\n    |\u003E Option.bind (fun xs -\u003E xs.TryFind item)\n\ncountOfItemInGroup Positive itemFrequencyByGroup \u0022sales\u0022\ncountOfItemInGroup Positive itemOccurenceByGroup \u0022sales\u0022\n\n(**\nWhile the frequency of each text item or word is simply its total count across all documents, an item\u0027s occurence is equivalent to the total count of *documents* that include text item *j*.\n\nWe can then define two variants of screening scores:\n\n1. Screening score based on total word frequency:\n\n$$f_{j} = \\frac{{\\text{count of word } j \\text{ in articles with } sgn(y) = \u002B1 }}{\\text{count of word } j \\text{ in all articles}} $$\n\n2. Screening score based on word occurence across documents:\n\n$$f_{j}^{*} = \\frac{{\\text{count of articles including word } j \\text{ in articles with } sgn(y) = \u002B1 }}{\\text{count of articles including word } j }$$\n*)\n\ntype CountType = \n    | Frequency\n    | Occurence\n\ntype TextItemScreening =\n    { TextItem: string\n      Score: float\n      Count: float \n      CountType: CountType }\n\n/// Vocabulary (training set only)\nlet vocabulary =\n    trainCalls\n    |\u003E Seq.collect (fun xs -\u003E xs.WordCount |\u003E Array.map (fun xs -\u003E xs.Word))\n    |\u003E Seq.distinct\n    |\u003E Seq.toArray\n\n/// Get scores from given word sentiment map (Frequency or Occurence)\nlet getScores (wordSentimentMap: Map\u003CSentiment, Map\u003Cstring, int\u003E\u003E)\n              (countType: CountType) = \n\n    let getItemScore item = \n        let generateItemScore item score count = \n            {TextItem = item; Score = score; Count = count; CountType = countType }\n\n        let posN, negN = \n            countOfItemInGroup Positive wordSentimentMap item,\n            countOfItemInGroup Negative wordSentimentMap item \n\n        match posN, negN with\n        | Some p, Some n -\u003E \n            let count = float (p \u002B n)\n            let score = (float p) / count\n            Some (generateItemScore item score count)\n        | Some p, None -\u003E \n            let score, count = 1., float p\n            Some (generateItemScore item score count)\n        | None, Some n -\u003E \n            let score, count = 0., float n\n            Some (generateItemScore item score count)\n        | _ -\u003E  None\n    \n    vocabulary\n    // Compute text item scores\n    |\u003E Array.Parallel.choose getItemScore\n    |\u003E Array.map (fun xs -\u003E xs.TextItem, xs)\n    |\u003E Map\n\nlet itemOccurenceScores, itemFrequencyScores = \n    getScores itemOccurenceByGroup Occurence, \n    getScores itemFrequencyByGroup Frequency\n\n(**\nHistogram: Item scores\n*)\n\nitemFrequencyScores\n|\u003E Map.toArray\n|\u003E Array.map (fun (word, xs) -\u003E xs.Score)\n|\u003E Array.filter (fun xs -\u003E xs \u003E 0.25 \u0026\u0026 xs \u003C 0.75)\n|\u003E Chart.Histogram\n|\u003E Chart.Show\n\n(**\n#### 1B) Sentiment-charged set of words\n\n$$\\widehat{S} = \\{j: f_{j} \\geq \\widehat{\\pi} \u002B \\alpha_{\u002B}, \\text{ or } f_{j} \\leq \\widehat{\\pi} - \\alpha_{-} \\} \\cap \\{ j: k_{j} \\geq \\kappa\\}$$\n\n- $f_{j} = \\text{Sentiment-screening score of word } j $\n- $\\widehat{\\pi} = \\text{Fraction of articles tagged with a positive return}$\n- $\\alpha_{\u002B} = \\text{Upper sentiment-score threshold}$\n- $\\alpha_{-} = \\text{Lower sentiment-score threshold}$\n- $k_{j} = \\text{count of word } j \\text{ in all articles}$\n\nThe thresholds ($\\alpha{\u002B}, \\alpha{-}, \\kappa$) are *hyper-parameters* that can be tuned via cross-validation.\n*)\n\n/// Sentiment-charged words\nlet getChargedItems alphaLower alphaUpper kappaPct = \n\n    // Count of text item in all articles\n    let kappa = kappaPct * float train.Length\n\n    // Upper and lower score thresholds\n    let upperThresh, lowerThresh = \n        trainCalls\n        |\u003E Array.filter (fun xs -\u003E xs.Flag = Positive)\n        |\u003E fun xs -\u003E float xs.Length / float train.Length\n        |\u003E fun pieHat -\u003E (pieHat \u002B alphaUpper), (pieHat - alphaLower)\n    \n    let isCharged item = \n        match itemFrequencyScores.TryFind item, itemOccurenceScores.TryFind item with\n        | Some freqScore, Some occScore -\u003E \n            if ((freqScore.Score \u003E= upperThresh || freqScore.Score \u003C= lowerThresh) \u0026\u0026 (occScore.Count \u003E= kappa))\n            then Some item\n            else None\n        | _ -\u003E None\n\n    vocabulary\n    |\u003E Array.choose isCharged\n\nlet alphaLower, alphaUpper, kappa  = (0.0075, 0.0075, 0.5)\nlet chargedItems = getChargedItems alphaLower alphaUpper kappa\n\nchargedItems.Length\n\n(**\n#### Filtering original item counts\n- Filter train and test\n*)\n\nlet filterCall (call: Call): Call = \n\n    let textItemsFromCall = \n        call.WordCount\n        |\u003E Array.map (fun xs -\u003E xs.Word, xs)\n        |\u003E Map\n\n    let filteredItemCounts = \n        chargedItems\n        |\u003E Array.map (fun chargedWord -\u003E \n            match textItemsFromCall.TryFind chargedWord with\n            | Some wordCount -\u003E wordCount\n            | None -\u003E {Word=chargedWord; Count = 0})\n        |\u003E Array.sortBy (fun xs -\u003E xs.Word)\n    \n    { CallId = call.CallId\n      WordCount = filteredItemCounts\n      Signal = call.Signal }\n\nlet chargedTrain = \n    trainCalls\n    |\u003E Array.Parallel.map filterCall\n    // Remove \u0022empty document vectors\u0022\n    |\u003E Array.filter (fun xs -\u003E xs.WordCount |\u003E Array.sumBy (fun xs -\u003E xs.Count) |\u003E fun xs -\u003E xs \u003C\u003E 0)\n\nlet getDocumentTermMatrix (calls: Call []) = \n    calls\n    |\u003E Array.sortBy (fun xs -\u003E xs.Signal)\n    |\u003E Array.map (fun xs -\u003E \n        xs.WordCount \n        |\u003E Array.map (fun xs -\u003E double xs.Count))\n    |\u003E matrix\n    |\u003E fun xs -\u003E xs.Transpose()\n\nlet chargedDocumentTermMatrix = getDocumentTermMatrix chargedTrain\n\nchargedDocumentTermMatrix.RowCount\nchargedDocumentTermMatrix.ColumnCount\n\n(**\n## 2. Learning Sentiment Topics\n*)\n\n(**\nFitting a two-topic model to the sentiment-charged counts, \u0060chargedItemCountsById\u0060.\n\nSome notation:\n\n$$\\text{Consider a collection of } n \\text{ articles and a dictionary of } m \\text{ words.}$$\n\n$$d_{i} = \\text{word or (phrase) counts of the } i^{th} article$$\n\n$$d_{i, j} = \\text{ number of times word } j \\text{ occurs in article } i$$\n\n$$D = m \\times n \\text{ document term matrix}; D = [d_{1}, ..., d{n}]$$\n\nModel:\n\n$$d_{[S], i} \\sim \\text{Multinomial} (s_{i}, p_{i}O_{\u002B} \u002B (1 - p_{i})O_{-})$$\n\n$$p_{i} = \\text{ article\u0027s sentiment score, } p_{i} \\in [0,1]$$\n\n$$s_{i} = \\text{ total count of sentiment-charged words in article } i$$\n\n$$O_{\u002B} = \\text{ positive sentiment topic}$$\n\n$$O_{-} = \\text{ negative sentiment topic}$$\n\n$$\\mathbb{E}h_{i} = \\mathbb{E}\\frac{d_{[S], i}}{s_{i}} = p_{i}O_{\u002B} \u002B (1 -p_{i})O_{-}$$\n\nEstimate $$H$$ by plugging in $$\\widehat{S}$$ from screening step:\n\n$$\\widehat{h_{i}} = \\frac{d_{[\\widehat{S}], i}}{\\widehat{s}_{i}}$$\n\n$$\\widehat{s}_{i} = \\sum_{j \\in \\widehat{S}}{d_{j, i}}$$\n\nEstimate W using the standardized ranks of returns. For each each article $$i$$ in the training sample $$i = 1, ..., n$$:\n\n$$\\widehat{p}_{i} = \\frac{\\text{rank of } y_{i} \\text{ in } \\{y_{l}\\}_{l=1}^{n}}{n}$$\n\n*)\n\n(**\n## Estimator $\\widehat{O}$\n*)\n\n(**\n#### $H$\n*)\n\n(**\n$$\\widehat{H} = [\\widehat{h_{1}}, \\widehat{h_{2}},..., \\widehat{h_{3}}]$$\n\n$$\\widehat{h_{i}} = \\frac{d_{[\\widehat{S}], i}}{\\widehat{s}_{i}} \\text{      } \\widehat{s}_{i} = \\sum_{j \\in \\widehat{S}}{d_{j, i}}$$\n*)\n\nlet bigH = \n    chargedDocumentTermMatrix\n    |\u003E fun m -\u003E\n        m.ToColumnArrays()\n        |\u003E Array.map (fun itemCounts -\u003E \n            let sumOfItemCounts = \n                Array.sum itemCounts \n\n            itemCounts \n            |\u003E Array.map (fun xs -\u003E xs / sumOfItemCounts))\n        |\u003E matrix\n        |\u003E fun xs -\u003E xs.Transpose()\n\nbigH.RowCount\nbigH.ColumnCount\n\n(**\n#### $W$\n*)\n\n(**\n$$\\widehat{W} = \\begin{bmatrix} \\widehat{p_{1}} \u0026 \\widehat{p_{2}} \u0026 \\cdots \u0026 \\widehat{p_{n}} \\\\ 1 - \\widehat{p_{1}} \u0026 1 - \\widehat{p_{2}} \u0026 \\cdots \u0026 1 -\\widehat{p_{n}} \\end{bmatrix}$$\n\n$$\\widehat{p}_{i} = \\frac{\\text{rank of } y_{i} \\text{ in } \\{y_{l}\\}_{l=1}^{n}}{n}$$\n*)\n\nlet bigW = \n    let n = \n        double chargedDocumentTermMatrix.ColumnCount\n\n    chargedDocumentTermMatrix.ToColumnArrays()\n    |\u003E Array.mapi (fun i _ -\u003E \n        double (i \u002B 1)/ n)\n    |\u003E fun xs -\u003E \n        matrix [|xs; xs |\u003E Array.map (fun p -\u003E (1. - p))|]\n\nbigW.RowCount\nbigW.ColumnCount\n\nbigH.Multiply(bigW.Transpose())\n\n(**\n#### $O$\n*)          \n\n(**\n$$\\widehat{O} = [\\widehat{h_{1}}, \\widehat{h_{2}},\\ldots, \\widehat{h_{n}}] \\widehat{W}^{\u0027} (\\widehat{W}\\widehat{W}^{\u0027})^{-1}$$\n*)\n\nlet bigO = \n    \n    let h, w, w\u0027 = bigH, bigW, bigW.Transpose()\n    let ww\u0027 = w.Multiply(w\u0027)\n\n    h.Multiply(w\u0027).Multiply(ww\u0027.Inverse())\n    |\u003E Matrix.toColArrays\n    |\u003E Array.map (fun col -\u003E \n        col\n        |\u003E Array.map (fun xs -\u003E if xs \u003C 0. then 0. else xs)\n        |\u003E fun onlyPositiveVals -\u003E \n            let norm = Array.sum onlyPositiveVals\n            onlyPositiveVals \n            |\u003E Array.map (fun xs -\u003E xs / norm))\n    |\u003E matrix\n    |\u003E fun m -\u003E m.Transpose()\n\nbigO.RowCount\nbigO.ColumnCount\n\n(**\n## 3. Scoring New Articles\n*)\n\n(**\nEstimating $p$ (sentiment score) for new articles using maximum likelihood estimation:\n\n$$\\widehat{p} = \\arg\\max_{p\\in[\\,0, 1]\\,} \\left\\{\\hat{s}^{-1} \\sum_{j \\in \\widehat{S}}{d_{j}\\log \\left(p \\widehat{O}_{\u002B, j} \u002B (\\,1-p)\\,\\widehat{O}_{-, j}\\right) \u002B \\lambda \\log \\left(p\\left(1 - p \\right)\\right) \\right\\}$$\n\n$\\hat{s}\\text{ is the total count of words from } \\widehat{S} \\text{ in the new article,} \\left(d_{j}, \\widehat{O}_{\u002B, j},  \\widehat{O}_{-, j} \\right) \\text{ are the } j \\text{th entries of the corresponding vectors, and } \\lamda $\n\nFor sentiment charged words, their corresponding entries in $O$ should be different. Otherwise, these words would not represent any sentiment and should be left out of the set of sentiment charged words. Sentiment neutral words are analogous to useless predictors in a linear model.\n*)\n\n(**\nOptimization\n*)\n\nlet bigOArr = \n    bigO.ToRowArrays()\n\nlet objF (call: Call) (p: float) (lambda: float) = \n\n    let filteredCall = filterCall call\n    \n    let sHat = \n        filteredCall\n        |\u003E fun xs -\u003E \n            xs.WordCount \n            |\u003E Array.sumBy (fun xs -\u003E xs.Count) \n            |\u003E fun xs -\u003E (1. / float xs)\n\n    filteredCall.WordCount\n    |\u003E Array.mapi (fun i xs -\u003E \n        let pos = p * bigOArr.[i].[0]\n        let neg = (1. - p) * bigOArr.[i].[1]\n        let d = float xs.Count\n        d * log (pos \u002B neg))\n    |\u003E fun expr -\u003E\n        (sHat *(Array.sum expr))\n        \nlet computeScore (call: Call) = \n    [|0. .. 0.01 .. 1.|]\n    |\u003E Array.map (fun scoreP -\u003E (scoreP, call.Signal), (objF call scoreP 0.00001))\n    |\u003E Array.maxBy snd\n\nlet testHighSignals = \n    testCalls\n    |\u003E Array.take 100\n    |\u003E Array.map (fun xs -\u003E \n        let res = computeScore xs\n        res)\n    |\u003E Array.sortBy snd\n\n\nlet testLowSignals = \n    testCalls\n    |\u003E Array.sortBy (fun xs -\u003E xs.Signal)\n    |\u003E Array.take 500\n    |\u003E Array.averageBy (fun xs -\u003E \n        let res = computeScore xs |\u003E fst\n        res |\u003E fst)\n\n(**\nDiffSharp demo\n*)\n\n#r \u0022nuget: DiffSharp-lite, 1.0.0-preview-987646120\u0022\n\nopen DiffSharp\nopen DiffSharp.Optim\n\nlet computeScore\u0027 (call: Call) (x: Tensor) =\n\n    let filteredCall = filterCall call\n\n    let sHat = \n        filteredCall\n        |\u003E fun xs -\u003E \n            xs.WordCount \n            |\u003E Array.sumBy (fun xs -\u003E xs.Count) \n            |\u003E fun xs -\u003E (1. / float xs)\n\n    let scoreP = x.[0]\n\n    filteredCall.WordCount\n    |\u003E Array.mapi (fun i xs -\u003E \n        let pos = scoreP * bigOArr.[i].[0]\n        let neg = (1. - scoreP) * bigOArr.[i].[1]\n        let d = float xs.Count\n        d * log (pos \u002B neg))\n    |\u003E Array.sum\n    |\u003E fun sumExpr -\u003E \n        (sHat * sumExpr) \u002B (0.001 * log (scoreP * (1. - scoreP)))\n        |\u003E fun xs -\u003E xs * -1.\n    \nlet lr, momentum, iters, threshold = 1e-3, 0.5, 1000, 1e-3\n\nlet scoreFun = computeScore\u0027 negativeArticleTrain\n\nlet scorePGuess = dsharp.tensor([0.5])\n\nlet scoreFx, params\u0027 = optim.sgd(scoreFun, scorePGuess, lr=dsharp.tensor(lr), momentum=dsharp.tensor(momentum), nesterov=true, iters=iters, threshold=threshold)"},{"uri":"/ConferenceCalls/TranscriptParsing.html","title":"Parsing Motley Fool","content":"(**\n---\ntitle: Parsing Motley Fool\ncategory: Scripts\ncategoryindex: 2\nindex: 1\n---\n\n[![Script](img/badge-script.svg)]({{root}}/{{fsdocs-source-basename}}.fsx)\u0026emsp;\n[![Notebook](img/badge-notebook.svg)]({{root}}/{{fsdocs-source-basename}}.ipynb)\n\n*)\n\n(**\n# Transcript Parsing\n*)\n\n(**\nThe objective of this \u0060TranscriptParsing.fsx\u0060 script is to give a few examples\non how to parse html documents with F#. More specifically, we will be attempting\nto parse earnings call transcripts from [Motley Fool](https://www.fool.com).\n\nBefore getting started, lets download the [FSharp.Data](https://fsprojects.github.io/FSharp.Data/)\nnuget package using .NET\u0027s package manager [NuGet](https://www.nuget.org/packages/FSharp.Data/):\n*)\n\n#r \u0022nuget: FSharp.Data\u0022\n\nopen System\nopen FSharp.Data\n\n(**\n## Transcript - Url\n*)\n\n(**\nWe can download or parse individual html documents with their url.\nSince each call transcript will have a different url, we need\nto find an effective and consistent way to fetch individual urls \nfrom motley fool\u0027s website. Fortunately, if we take a look at \u003Ca href=\u0022https://www.fool.com/earnings-call-transcripts/?page=1\u0022 target=\u0022_blank\u0022\u003Emotley fool\u0027s front page\u003C/a\u003E, we see that all call transcripts are tagged with hyperlinks. \n*)\n\n(**\n\u003Cimg src=\u0022FsdocsImages\\motley_fool_front_page.png\u0022 width=\u002270%\u0022 \u003E\n*)\n\n(**\nSince the transcripts are tagged with a specific hypertext reference \n(href) (\u0060\u0022/earnings/call-transcripts\u0022\u0060), we can use the \u0060CssSelect\u0060 \nmethod from FSharp Data to find all elements in a given front page \nthat match the transcript href that we are looking for. After fetching \nthe urls, we can download any transcript we want as an html document \nusing the \u0060HtmlDocument.Load\u0060 method, also from FSharp Data.\n*)\n\ntype FrontPageDocument = HtmlDocument\n\n/// Match html node with \u0022href\u0022 attribute and create transcript url\nlet makeFoolUrl (attrib:HtmlAttribute) = \n    match attrib.Name(), attrib.Value() with\n    | \u0022href\u0022, stub -\u003E $\u0022https://www.fool.com{stub}\u0022\n    | _, _ -\u003E failwithf $\u0022Expected href attribute but got {attrib}\u0022\n\n/// Search for transcript urls\nlet findTranscriptUrls (pageDoc: FrontPageDocument): string [] =  \n    pageDoc.CssSelect(\u0022a[href^=\u0027/earnings/call-transcripts\u0027]\u0022)\n    |\u003E Seq.choose (HtmlNode.tryGetAttribute \u0022href\u0022)\n    |\u003E Seq.map makeFoolUrl\n    |\u003E Seq.toArray\n    \n(** \nLets take a look at the first three call transcript urls \u0060CssSelect\u0060 was able to match:\n*)\n\nlet exampleFrontPageDoc: FrontPageDocument = HtmlDocument.Load \u0022https://www.fool.com/earnings-call-transcripts/?page=1\u0022\n\nlet exampleUrls = findTranscriptUrls exampleFrontPageDoc\n\n/// First three urls\nexampleUrls\n|\u003E Array.take 3\n|\u003E Array.iter (fun xs -\u003E printfn$\u0022{xs}\u0022)\n\n(*** include-fsi-output***)\n\n(**\n## Transcript - Ticker \u0026 Exchange\n*)\n\n(** \nApart from using the \u0060CssSelect\u0060 method to search for transcript urls \nwe can also use it to extract other key information like a company\u0027s \nticker and exchange as well as the time and date of the earnings call.\n\nLets see if we can fetch Tesla\u0027s ticker and exchange from its \n[2021 Q2 earnings call](https://www.fool.com/earnings/call-transcripts/2021/07/27/tesla-tsla-q2-2021-earnings-call-transcript/): \n\n\u003Cimg src=\u0022FsdocsImages\\tesla_motley_fool.png\u0022 width=\u002270%\u0022\u003E\n*) \n\ntype TranscriptDocument = HtmlDocument\n/// Tesla transcript html document\nlet teslaDoc: TranscriptDocument = HtmlDocument.Load \u0022https://www.fool.com/earnings/call-transcripts/2021/07/27/tesla-tsla-q2-2021-earnings-call-transcript/\u0022\n\nteslaDoc.CssSelect(\u0022span[class=\u0027ticker\u0027]\u0022)\n(***include-fsi-output***)\n\nteslaDoc.CssSelect(\u0022span[class=\u0027ticker\u0027]\u0022)\n|\u003E List.map (fun x -\u003E x.InnerText())\n(***include-fsi-output***)\n\nteslaDoc.CssSelect(\u0022span[class=\u0027ticker\u0027]\u0022)\n|\u003E List.map (fun x -\u003E \n    x.InnerText()\n     .Trim()\n     .Replace(\u0022(\u0022,\u0022\u0022)\n     .Replace(\u0022)\u0022,\u0022\u0022))\n|\u003E List.distinct\n|\u003E List.tryExactlyOne     \n(***include-fsi-output***)\n\n// A function to do the same\nlet cleanTickerExchangeText (doc:TranscriptDocument) =\n    doc.CssSelect(\u0022span[class=\u0027ticker\u0027]\u0022)\n    |\u003E Seq.map (fun x -\u003E \n        x.InnerText()\n         .Trim()\n         .Replace(\u0022(\u0022,\u0022\u0022)\n         .Replace(\u0022)\u0022,\u0022\u0022))\n    |\u003E Seq.distinct\n    |\u003E Seq.tryExactlyOne\n\ncleanTickerExchangeText teslaDoc\n(***include-fsi-output***)\n\n(**\nSince we are not certain that we\u0027ll retrieve both a ticker and an exchange \nfrom *every* single transcript we parse, we can use match expressions and \noption types to make sure to return only those matches that contain both a \nvalid ticker and exchange. \n*)\n\n/// Match inner text from html node to a ticker and exchange\nlet tryTickerExchange (tickerInfo: string): option\u003Cstring * string\u003E =\n    match tickerInfo.Split(\u0022:\u0022) with\n    |[|exchange; ticker|] -\u003E Some (ticker, exchange)\n    | _ -\u003E None\n\n/// Search for ticker and exchange\nlet findTickerExchange (doc: TranscriptDocument): option\u003Cstring * string\u003E = \n    doc\n    |\u003E cleanTickerExchangeText\n    |\u003E Option.bind tryTickerExchange\n\n// Tesla ticker and exchange\nfindTickerExchange teslaDoc\n\n(*** include-it ***)\n\n(**\n## Transcript - Date \u0026 Time\n*)\n\n(**\nTaking a closer look at Tesla\u0027s earnings transcript page, we can see that right \nbelow Tesla\u0027s ticker we spot the exact time and date of the earnings call.\n\nLet\u0027s see if we can use \u0060CssSelect\u0060 to fetch this information:\n*)\n\n(**\n### Date\n*)\n\n/// Format date string\nlet cleanDate (node: HtmlNode): option\u003Cstring\u003E = \n    node.InnerText()\n        .ToUpperInvariant()\n        .Replace(\u0022.\u0022, \u0022\u0022)\n        .Replace(\u0022,\u0022, \u0022\u0022)\n        .Trim()\n        .Split(\u0022 \u0022)\n    |\u003E fun dateArr -\u003E     \n        match dateArr with\n        |[|month; day; year|] -\u003E Some ($\u0022{month.[..2]} {day} {year}\u0022) \n        | _ -\u003E None\n\n/// Search for transcript date\nlet findDate (doc: TranscriptDocument): option\u003Cstring\u003E=\n    doc.CssSelect(\u0022span[id=\u0027date\u0027]\u0022)\n    |\u003E Seq.tryExactlyOne\n    |\u003E Option.bind cleanDate\n\n/// Date of Tesla\u0027s call:\nfindDate teslaDoc\n\n(*** include-it ***)\n\n(**\n### Time\n*)\n\n/// Format time string\nlet cleanTime (node: HtmlNode) =\n    node.InnerText()\n        .ToUpperInvariant()\n        .Replace(\u0022.\u0022, \u0022\u0022)\n    |\u003E fun txt -\u003E    \n        if (txt.Contains \u0022ET\u0022)\n        then txt.Replace(\u0022ET\u0022, \u0022\u0022).Trim()\n        else failwithf $\u0022Expected ET timezone but got {txt}\u0022 \n   \n/// Search for transcript time\nlet findTime (doc: TranscriptDocument) =\n    doc.CssSelect(\u0022em[id=\u0027time\u0027]\u0022)\n    |\u003E Seq.tryExactlyOne\n    |\u003E Option.map cleanTime\n\n/// Time of Tesla\u0027s call\nfindTime teslaDoc\n\n(*** include-it ***)\n\n(**\n### DateTime\n*)\n\n(**\nNow that we have working functions for both the date and time of each call, \nlets combine these functions together and convert the information we have on\nthe date and time of an earnings call to a \u003Ca href=\u0022https://docs.microsoft.com/en-us/dotnet/api/system.datetime?view=net-5.0\u0022 target=\u0022_blank\u0022\u003EDateTime struct\u003C/a\u003E :\n*)\n\n/// DateTime converter\nlet convertToDateTime (date: string, time: string): DateTime =\n    let dateExpr = $\u0022{date} {time}\u0022\n    let dateFormat = \u0022MMM d yyyy h:mm tt\u0022\n    DateTime.ParseExact(dateExpr, dateFormat, System.Globalization.CultureInfo.InvariantCulture)\n\n/// Search for and match date and time\nlet findDateTime (doc: TranscriptDocument): option\u003CDateTime\u003E =\n    match findDate doc, findTime doc with\n    | Some date, Some time -\u003E \n        let dt = convertToDateTime (date, time)\n        Some dt\n    | _ -\u003E None\n\n/// Tesla call DateTime\nfindDateTime teslaDoc\n\n(*** include-it ***)\n\n(**\n## Transcript - Paragraphs\n*)\n\n(**\nThe transcript itself can also be easily parsed using the \u0060CssSelect()\u0060 method.\nIn html, blocks of text or paragraphs are defined with the \u0022\u003Cp\u003E\u0022 tag: \n*)\n\nlet findParagraphs (doc: TranscriptDocument): string [] = \n    doc.CssSelect(\u0022p\u0022)\n    |\u003E Seq.map (fun x -\u003E x.InnerText().Trim())\n    // Remove empty paragraphs\n    |\u003E Seq.filter (fun x -\u003E x \u003C\u003E \u0022\u0022)\n    // Skip first 5 paragraphs\n    |\u003E Seq.skip 5\n    |\u003E Seq.toArray\n\nlet firstCharacters (paragraph: string) = \n    if paragraph.Length \u003C= 50 \n    then paragraph \n    else paragraph.[..49] \u002B \u0022 ... \u0022\n\n// First two paragraphs\nteslaDoc \n|\u003E findParagraphs\n|\u003E Array.take 5\n|\u003E Array.map firstCharacters\n|\u003E Array.iteri (printfn \u0022Paragraph %i: %s\u0022)\n\n(*** include-output***)\n\n\n(**\n## Transcript - Fiscal Quarter\n*)\n\n(**\nAlthough we have already found a way to fetch the exact time and date \nof each earnings call, we could also fetch from the title of each \ntranscript the quarter of which each call refers to.\n\nExample titles:\n\n- Microsoft (MSFT) *Q4* 2021 Earnings Call Transcript\n- Tesla (TSLA) *Q2* 2021 Earnings Call Transcript\n- IBM (IBM) *Q2* 2021 Earnings Call Transcript\n\nWe can already see a pattern emerging from the titles:\n\n- CompanyName (CompanyTicker) *Q[1,2,3,4]* 0000 Earnings Call Transcript \n\nHaving idendified this pattern, we can create a [Regular Expression](https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference)\nRegex) pattern to help us extract the fiscal quarter from each title.\n*)\n\nopen System.Text.RegularExpressions\n\n/// Regular Expression\nlet quarterRegex = Regex(\u0022Q\\d{1}\u0022)\n\n/// Extract number from \u0022Q\\d{1}\u0022\nlet getQNumb (q: string): option\u003Cint\u003E = \n    Seq.toArray q\n    |\u003E fun xs -\u003E\n        match xs with\n        | [|q; qNumb|] -\u003E Some (qNumb |\u003E Char.GetNumericValue |\u003E int)\n        | _ -\u003E None\n\nlet findFiscalQuarter (doc: TranscriptDocument): option\u003Cint\u003E = \n    doc.CssSelect(\u0022title\u0022)\n    |\u003E Seq.map (fun xs -\u003E \n        xs.InnerText() \n        |\u003E quarterRegex.Match \n        |\u003E fun xs -\u003E xs.Value)\n    // Check if there is exactly one match\n    |\u003E Seq.tryExactlyOne\n    // Convert string to int\n    |\u003E Option.bind getQNumb\n\nfindFiscalQuarter teslaDoc\n\n(*** include-output***)\n\n(**\n## EarnignsCall Record\n*)\n\n(**\nSo far we have worked with individual functions that take in one single argument, \nan html transcript document. Since they all work with the \u0060TranscriptDocument\u0060 \ntype, we can easily combine these functions together to form one single function \nthat returns all the individual bits of data that we want.\n\nWe\u0027ll use a record called \u0060EarningsCall\u0060 to hold all our information.\n*)\n\ntype CallId =\n    { Ticker: string \n      Exchange: string\n      Date: System.DateTime\n      FiscalQuarter : int }\n\ntype EarningsCall = \n    { CallId : CallId\n      Transcript: string [] }\n\n/// Search for ticker, exchange, date and paragraphs\nlet parseTrancriptDoc (doc: TranscriptDocument): option\u003CEarningsCall\u003E =\n    let matchExpr =  \n        findTickerExchange doc, \n        findDateTime doc,\n        findFiscalQuarter doc\n     \n    match matchExpr with\n    | Some (ticker, exchange), \n      Some date,\n      Some fiscalQuarter -\u003E \n        let callId = \n            { Ticker = ticker \n              Exchange = exchange\n              Date = date \n              FiscalQuarter = fiscalQuarter }\n        \n        Some { CallId = callId\n               Transcript = findParagraphs doc }\n    | _ -\u003E None\n\n/// Tesla transcript record\nlet teslaTranscript = parseTrancriptDoc teslaDoc\n\nteslaTranscript \n|\u003E Option.iter(fun xs -\u003E \n    printfn $\u0022Id:\\n{xs.CallId}\\n\u0022\n    printfn $\u0022First 5 paragraphs:\u0022\n    xs.Transcript\n    |\u003E Array.truncate 5\n    |\u003E Array.map firstCharacters \n    |\u003E Array.iter (printfn \u0022%A\u0022))\n(*** include-output ***)\n\n(**\nNow that we have a working function that takes in a \u0060TranscriptDocument\u0060 and\nreturns a \u0060EarningsCall\u0060 type, lets try to parse all of the transcript urls from \n\u0060exampleFrontPageDoc\u0060.\n*)\n\n/// Parsing transcripts from front page\nlet exampleTranscripts = \n    exampleFrontPageDoc\n    |\u003E findTranscriptUrls \n    |\u003E Array.choose (fun tUrl -\u003E \n        let doc = HtmlDocument.Load tUrl\n        parseTrancriptDoc doc)\n\n/// Total number of transcripts\nprintfn $\u0022N: {exampleTranscripts.Length}\u0022\n\n(*** include-output ***)\n\n/// First 5 transcripts\nexampleTranscripts\n|\u003E Array.take 5\n|\u003E Array.iter (fun xs -\u003E \n    let tId = xs.CallId\n    printfn $\u0022TranscriptId: %4s{tId.Ticker}, %6s{tId.Exchange}, {tId.Date}\u0022)\n\n(*** include-output ***)\n\n(**\n## Data visualization with Plotly.NET\n*)\n\n(**\n.NET has several useful libraries, including one dedicated for generating charts.\nWith [Plotly.NET](https://plotly.net/) you can create all sorts of charts from \nsimple histograms all the way to 3D surface plots. Just like with FSharp Data, \nwe can download Plotly.Net with .NET\u0027s package manager, Nuget.\n*)\n\n#r \u0022nuget: Plotly.NET, 2.0.0-preview.6\u0022\nopen Plotly.NET\n\n/// Histogram\nlet transcriptTimesHistogram = \n\n    let callTimes = \n        exampleTranscripts\n        |\u003E Array.map (fun xs -\u003E xs.CallId.Date.TimeOfDay.ToString())\n        |\u003E Array.sort\n    \n    callTimes\n    |\u003E Chart.Histogram\n    |\u003E Chart.withTitle \u0022Earnings calls by time of day (ET)\u0022\n    |\u003E Chart.withY_AxisStyle \u0022Count\u0022\n    |\u003E Chart.withSize (750., 500.)\n\n(*** do-not-eval ***)\ntranscriptTimesHistogram |\u003E Chart.Show \n(*** hide ***)\ntranscriptTimesHistogram |\u003E GenericChart.toChartHTML\n(*** include-it-raw ***)\n\n(**\nAlthough we are working with a small sample, we can already notice that \nthe time of the earnings calls are varied and that calls occur before \nmarket hours, during market hours and even after market hours.\n*)\n\n(**\n## Async methods\n*)\n\nlet asyncTranscript (url: string) = \n    let rec loop attempt url =\n        async {\n            try \n                let! transcriptDoc = HtmlDocument.AsyncLoad url\n                let transcriptRec = parseTrancriptDoc transcriptDoc\n                return transcriptRec\n            with e -\u003E\n                if attempt \u003E 0 then\n                    do! Async.Sleep 2000 // Wait 2 seconds in case we\u0027re throttled.\n                    return! loop (attempt - 1) url\n                else return! failwithf \u0022Failed to request \u0027%s\u0027. Error: %O\u0022 url e }\n    loop 5 url\n\nlet asyncPage (n: int) =\n    let rec loop attempt n =\n        async {\n            printfn $\u0022{n}\u0022\n            let frontPageP = $\u0022https://www.fool.com/earnings-call-transcripts/?page={n}\u0022\n            try \n                let! pageDoc = HtmlDocument.AsyncLoad frontPageP \n                return findTranscriptUrls pageDoc\n            with e -\u003E\n                if attempt \u003E 0 then\n                    do! Async.Sleep 2000 // Wait 2 seconds in case we\u0027re throttled.\n                    return! loop (attempt - 1) n\n                else return! failwithf \u0022Failed to request \u0027%s\u0027. Error: %O\u0022 frontPageP e }\n    loop 5 n \n    \n(**\n### Parse Transcript Pages\n*)\n\nmodule Async =\n    let ParallelThrottled xs = Async.Parallel(xs, 5)\n\nlet asyncPages (pages: int list) = \n    let urls = \n        pages \n        |\u003E Seq.map asyncPage\n        |\u003E Async.ParallelThrottled \n        |\u003E Async.RunSynchronously\n        |\u003E Array.collect id\n    let transcripts =\n        urls\n        |\u003E Array.map asyncTranscript\n        |\u003E Async.ParallelThrottled\n        |\u003E Async.RunSynchronously\n        |\u003E Array.choose id\n    transcripts\n\n(***do-not-eval***)\n// Done\nlet examplePages = asyncPages [1 .. 5]\n\n(**\n## Export to json\n*)\n\n#r \u0022nuget: Newtonsoft.Json\u0022\nopen Newtonsoft.Json\n\nEnvironment.CurrentDirectory \u003C- __SOURCE_DIRECTORY__\n\nlet transcriptsToJson (fileName: string) (calls: EarningsCall []) = \n    JsonConvert.SerializeObject(calls)\n    |\u003E fun json -\u003E IO.File.WriteAllText(fileName, json)\n\n(*** do-not-eval***)\ntranscriptsToJson \u0022data-cache/examplePages.json\u0022 examplePages"},{"uri":"/ConferenceCalls/index.html","title":"Introduction\n","content":"# Introduction\n\nThe focus of this repository is to take you through a series of *text-mining* methods \nin Finance with an ultimate objective of conducting *sentiment analysis*.\n\nIn particular, we\u0027ll be exploring these methods on a collection of earnings \ncalls transcripts that were made available to us by [Motley Fool](https://www.fool.com), \nan online financial advisor. It is not known when the actual earnings announcements\ntook place, but it is assumed that the calls occured either at the same time or \nright after the earnings announcements. \n\nEssentially, the objective of an earnings call is to provide investors with \ninformation regarding a company\u0027s past performance and future plans/projects, \nas well as answering questions from the media and related personnel. With \ncompany executives discussing pertinent matters related to the company\u0027s financial \nperformace and general outlook. On a more technical note, earnings calls play a role \nin updating investors beliefs towards a given company, therefore allowing them to make\nbetter informed investment decisions. For this very reason, earnings calls \nare particularly valuable for investors when relevant prior non-public information is discussed/revealed.\n\nExample earnings call:\n\n\u003Cimg src=\u0022FsdocsImages\\tesla_motley_fool.png\u0022 width=\u002270%\u0022\u003E\n\nThe field of text-mining is incredibly diverse and can be segmented into seven practice areas:\n\n1.\tSearch and information retrieval\n\n2.\tDocument clustering\n\n3.\tDocument classification \n\n4.\tWeb mining\n\n5.\tInformation extraction\n\n6.\tNatural Language Processing (NLP)\n\n7.\tConcept extraction\n\nWhile all seven practice areas are very much in use in differing areas within \neconomics and finance, a significant amount of research today is focused on \ndocument classification, information retrieval and NLP. \n\n*Sentiment analysis* is considered to fall under the umbrella of \ndocument classification. In a nutshell, it seeks to determine general sentiments,\nopinions, and affective states of people reflected in a corpus of text.\n\nSome well-known classification algorithms: \n\n- Naive Bayes\n- Logistic Regression\n- Decision trees\n- Neural Network"},{"uri":"/ConferenceCalls/Common.html","title":"Common","content":"/// #load \u0022C:\\Users\\Five star\\Documents\\GitHub\\ConferenceCalls\\Secrets.fsx\u0022\n/// #load \u0022/Users/antonioelias/Documents/GitHub/ConferenceCalls/Secrets.fsx\u0022\n\n#r \u0022nuget: FSharp.Data\u0022\n#load \u0022C:\\Users\\Five star\\Documents\\GitHub\\ConferenceCalls\\Secrets.fsx\u0022\n\nopen System\nopen System.IO\nopen FSharp.Data\n\ntype Frequency = Daily | Monthly\ntype ReturnObs = { Symbol: string; Date: DateTime; Return : float }\n\nmodule Tiingo =\n\n    type TiingoCsv = CsvProvider\u003C\u0022date,close,high,low,open,volume,adjClose,adjHigh,adjLow,adjOpen,adjVolume,divCash,splitFactor\n2020-10-01,9.77,10.25,9.69,10.09,4554055,9.77,10.25,9.69,10.09,4554055.0,0.0,1.0\u0022\u003E\n\n    type TiingoRequest = { Symbol : string; Start : DateTime; End : DateTime }\n\n    type TiingoObs =\n        {\n            Date : DateTime\n            Close : decimal\n            High : decimal\n            Low : decimal\n            Open : decimal \n            Volume : int\n            AdjClose : decimal\n            AdjHigh : decimal\n            AdjLow : decimal\n            AdjOpen : decimal\n            AdjVolume : decimal\n            DivCash : decimal\n            SplitFactor : decimal\n        }\n\n    ///\u003Csummary\u003EConstructs a Tiingo request. By default is to get the past year of data.\u003C/summary\u003E\n        /// \u003Cparam name=\u0022symbol\u0022\u003EThe ticker symbol such as \u0022AAPL\u0022,\u0022MSFT\u0022 etc.\u003C/param\u003E\n    let request symbol = { Symbol = symbol; Start = DateTime.Now.AddYears(-1); End = DateTime.Now}\n    ///\u003Csummary\u003ESets the Tiingo request start date.\u003C/summary\u003E\n        /// \u003Cparam name=\u0022startOn\u0022\u003ERequest start date\u003C/param\u003E\n        /// \u003Cparam name=\u0022request\u0022\u003EThe Tiingo request to update.\u003C/param\u003E\n    let startOn startOn request = { request with Start = startOn }\n    ///\u003Csummary\u003ESets the Tiingo request end date.\u003C/summary\u003E\n        /// \u003Cparam name=\u0022endOn\u0022\u003ERequest start date\u003C/param\u003E\n        /// \u003Cparam name=\u0022request\u0022\u003EThe Tiingo request to update.\u003C/param\u003E\n    let endOn endOn request = { request with End = endOn }\n    \n    let private cache = Runtime.Caching.createInMemoryCache (TimeSpan(hours=12,minutes=0,seconds=0))\n\n    ///\u003Csummary\u003EDownloads Tiingo data.\u003C/summary\u003E\n        /// \u003Cparam name=\u0022request\u0022\u003EThe Tiingo request to download.\u003C/param\u003E\n    let get request =\n        let dtStr (x : DateTime) = x.Date.ToString(\u0022yyyy-MM-dd\u0022)\n        let request = { request with Start = request.Start.Date; End = request.End.Date }\n        let key = $\u0022{request.Symbol}-{dtStr request.Start}-{dtStr request.End}.csv\u0022\n        match cache.TryRetrieve(key) with\n        | Some res -\u003E res\n        | None -\u003E\n            let result = \n                Http.RequestString\n                            ( $\u0022https://api.tiingo.com/tiingo/daily/{request.Symbol}/prices\u0022, \n                                httpMethod = \u0022GET\u0022,\n                                query   = [ \u0022token\u0022, Secrets.tiingoKey; \n                                            \u0022startDate\u0022, request.Start.ToString(\u0022yyyy-MM-dd\u0022);\n                                            \u0022endDate\u0022, request.End.ToString(\u0022yyyy-MM-dd\u0022);\n                                            \u0022format\u0022,\u0022csv\u0022],\n                                headers = [HttpRequestHeaders.Accept HttpContentTypes.Csv])\n            cache.Set(key,result)\n            result\n        |\u003E TiingoCsv.Parse\n        |\u003E fun parsed -\u003E\n            parsed.Rows\n            |\u003E Seq.map(fun row -\u003E\n                { Date = row.Date\n                  Close = row.Close\n                  High = row.High\n                  Low = row.Low\n                  Open = row.Open\n                  Volume = row.Volume\n                  AdjClose = row.AdjClose\n                  AdjHigh = row.AdjHigh\n                  AdjLow = row.AdjLow\n                  AdjOpen = row.AdjOpen\n                  AdjVolume = row.AdjVolume\n                  DivCash = row.DivCash\n                  SplitFactor = row.SplitFactor \n                  })\n            |\u003E Seq.toArray      \n    \n    // using a class, keeping private for now.\n    type private Download(symbol:string,?startOn:DateTime,?endOn:DateTime) =\n        let startOn = defaultArg startOn (DateTime.Now.AddYears(-1))\n        let endOn = defaultArg endOn (DateTime.Now)\n        let data = get { Symbol = symbol; Start = startOn; End = endOn }\n        member this.Rows = data\n \n    // Probably deprecated\n    let private getFromCacheDirectory cacheDirectory request =\n        let dtStr (x : DateTime) = x.Date.ToString(\u0022yyyy-MM-dd\u0022)\n        let request = { request with Start = request.Start.Date; End = request.End.Date }\n        let key = $\u0022{request.Symbol}-{dtStr request.Start}-{dtStr request.End}.csv\u0022\n        let cacheFile = cacheDirectory \u002B key\n        if File.Exists(cacheFile) then\n            File.ReadAllText(cacheFile)\n        else    \n            let result = \n                Http.RequestString\n                            ( $\u0022https://api.tiingo.com/tiingo/daily/{request.Symbol}/prices\u0022, \n                                httpMethod = \u0022GET\u0022,\n                                query   = [ \u0022token\u0022, Secrets.tiingoKey; \n                                            \u0022startDate\u0022, request.Start.ToString(\u0022yyyy-MM-dd\u0022);\n                                            \u0022endDate\u0022, request.End.ToString(\u0022yyyy-MM-dd\u0022);\n                                            \u0022format\u0022,\u0022csv\u0022],\n                                headers = [HttpRequestHeaders.Accept HttpContentTypes.Csv])\n            File.WriteAllText(cacheFile,result)\n            result\n        |\u003E TiingoCsv.Parse\n    \n    let private returnHelper symbol (xs:TiingoObs seq) =\n        xs\n        |\u003E Seq.sortBy(fun x -\u003E x.Date)\n        |\u003E Seq.pairwise\n        |\u003E Seq.map(fun (yesterday, today) -\u003E\n            { Symbol = symbol \n              Date = today.Date\n              Return =  float (today.AdjClose / yesterday.AdjClose) - 1.0})\n        |\u003E Seq.toArray      \n\n    let getReturns request =\n        get request\n        |\u003E (returnHelper request.Symbol)\n\n    // Marking as private so people don\u0027t use it by accident\n    let private getInternetFileCache request =\n        let cache = Runtime.Caching.createInternetFileCache \u0022tiingo\u0022 (TimeSpan.FromDays 30.0)\n        let request = { request with Start = request.Start.Date; End = request.End.Date }\n        let key = request.ToString()\n        match cache.TryRetrieve(key) with\n        | Some res -\u003E res\n        | None -\u003E\n            let res =\n                Http.RequestString\n                        ( $\u0022https://api.tiingo.com/tiingo/daily/{request.Symbol}/prices\u0022, \n                            httpMethod = \u0022GET\u0022,\n                            query   = [ \u0022token\u0022, Secrets.tiingoKey; \n                                        \u0022startDate\u0022, request.Start.ToString(\u0022yyyy-MM-dd\u0022);\n                                        \u0022endDate\u0022, request.End.ToString(\u0022yyyy-MM-dd\u0022);\n                                        \u0022format\u0022,\u0022csv\u0022],\n                            headers = [HttpRequestHeaders.Accept HttpContentTypes.Csv ])\n            cache.Set(key, res)\n            res\n        |\u003E TiingoCsv.Parse\n\nmodule French =\n    //open System.Net\n    open System.IO.Compression\n\n    type private FF3Csv = CsvProvider\u003C\u0022Date (string),Mkt-RF,SMB,HML,RF\n        19260701,    0.10,   -0.24,   -0.28,   0.009\u0022\u003E\n    \n    type FF3Obs = \n        { Date : DateTime \n          MktRf : float\n          Smb : float \n          Hml : float\n          Rf : float \n          Frequency : Frequency } \n          \n    let private frenchDay x = \n        DateTime.ParseExact(x,\n            \u0022yyyyMMdd\u0022,\n            Globalization.CultureInfo.InvariantCulture)\n    let private frenchMonth x = \n        DateTime.ParseExact(x,\n            \u0022yyyyMM\u0022,\n            Globalization.CultureInfo.InvariantCulture)\n\n    let private cache = \n        let today = DateTime.Now\n        let nextMonth = today.AddMonths(1)\n        let eom = DateTime(nextMonth.Year, nextMonth.Month, 1).AddDays(-1.0) \n        Runtime.Caching.createInternetFileCache \u0022French\u0022 (eom - today)\n\n    let private getData (dataset:string) =\n        match cache.TryRetrieve(dataset) with\n        | Some data -\u003E data\n        | None -\u003E\n            //let dataset = \u0022F-F_Research_Data_Factors_CSV\u0022\n            let urlString = $\u0022http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/{dataset}.zip\u0022\n            let request = Http.RequestStream(urlString, httpMethod = \u0022GET\u0022,headers = [HttpRequestHeaders.Accept HttpContentTypes.Any])\n            use archive = new ZipArchive(request.ResponseStream,ZipArchiveMode.Read)\n            let file = archive.GetEntry($\u0022{dataset}\u0022.Replace(\u0022_CSV\u0022,\u0022.CSV\u0022))\n            use reader = new StreamReader(file.Open())\n            let data  = reader.ReadToEnd()\n            cache.Set(dataset,data)\n            data\n    let getFF3 frequency =\n            let (dataset, dateParser) =\n                match frequency with\n                | Monthly -\u003E \u0022F-F_Research_Data_Factors_CSV\u0022, frenchMonth\n                | Daily -\u003E \u0022F-F_Research_Data_Factors_daily_CSV\u0022, frenchDay\n            let data = new StringReader(getData dataset)\n            data.ReadToEnd().Split(\u0022\\r\\n\u0022)\n            |\u003E Array.skipWhile(fun line -\u003E not (line.Contains(\u0022Mkt-RF\u0022)))\n            |\u003E Array.skip 1\n            |\u003E Array.takeWhile(fun line -\u003E line \u003C\u003E \u0022\u0022)\n            |\u003E Array.map(fun line -\u003E \n                let parsedLine = FF3Csv.ParseRows(line).[0] \n                { Date = dateParser parsedLine.Date\n                  MktRf = float parsedLine.\u0060\u0060Mkt-RF\u0060\u0060 / 100.0\n                  Smb = float parsedLine.SMB / 100.0\n                  Hml = float parsedLine.HML / 100.0\n                  Rf = float parsedLine.RF / 100.0 \n                  Frequency = Monthly })\n\n\n    "}]