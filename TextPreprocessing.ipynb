
        {
            "cells": [
          {
           "cell_type": "code",
           "metadata": {},
            "execution_count": 1, "outputs": [], 
           "source": ["namespace Preprocessing\n",
"\n",
"module Normalization = \n",
"\n",
"    open System.Text.RegularExpressions\n",
"\n",
"    // Detect sentence boundaries\n",
"    let splitParagraph (paragraph: string) = \n",
"        paragraph.Replace(\".\", \"XXXX\")\n",
"                 .Replace(\"?\", \"XXXX\")\n",
"                 .Replace(\"!\", \"XXXX\")\n",
"                 .Split(\"XXXX\") \n",
"        |\u003e Array.map (fun xs -\u003e xs.Replace(\"XXXX\", \"\").Trim())\n",
"        |\u003e Array.filter (fun xs -\u003e xs.Length \u003c\u003e 0)\n",
"\n",
"    /// Check for *only* words (Regex)       \n",
"    let getOnlyWords (text: string): string= \n",
"        let onlyWords = Regex(@\"(?\u003c!\\S)[a-zA-Z]\\S*[a-zA-Z](?!\\S)\")\n",
"\n",
"        text.Replace(\",\", \"\")\n",
"            .Replace(\";\", \"\")\n",
"            .Replace(\":\", \"\")\n",
"            .Trim()\n",
"            .ToLowerInvariant()\n",
"        |\u003e onlyWords.Matches\n",
"        |\u003e Seq.cast\u003cMatch\u003e\n",
"        |\u003e Seq.map (fun m -\u003e m.Value)\n",
"        |\u003e String.concat(\" \")\n",
"\n",
"    /// Non-exhaustive map of english contractions\n",
"    let englishContractions: Map\u003cstring, string\u003e= \n",
"        [\n",
"        (\"aren\u0027t\", \"are not\")\n",
"        (\"can\u0027t\", \"cannot\")\n",
"        (\"could\u0027ve\", \"could have\")\n",
"        (\"couldn\u0027t\", \"could not\")\n",
"        (\"dammit\", \"damn it\")\n",
"        (\"didn\u0027t\", \"did not\")\n",
"        (\"doesn\u0027t\", \"does not\")\n",
"        (\"don\u0027t\", \"do not\")\n",
"        (\"dunno\", \"do not know\")\n",
"        (\"everybody\u0027s\", \"everybody is\")\n",
"        (\"everyone\u0027s\", \"everyone is\")\n",
"        (\"gimme\", \"give me\")\n",
"        (\"gonna\", \"going to\")\n",
"        (\"gotta\", \"got to\")\n",
"        (\"hadn\u0027t\", \"had not\")\n",
"        (\"had\u0027ve\", \"had have\")\n",
"        (\"hasn\u0027t\", \"has not\")\n",
"        (\"haven\u0027t\", \"have not\")\n",
"        (\"here\u0027s\", \"here is\")\n",
"        (\"how\u0027ll\", \"how will\")\n",
"        (\"how\u0027re\", \"how are\")\n",
"        (\"i\u0027ll\", \"I will\")\n",
"        (\"i\u0027m\", \"I am\")\n",
"        (\"imma\", \"I am about to\")\n",
"        (\"innit\", \"is it not\")\n",
"        (\"i\u0027ve\", \"I have\")\n",
"        (\"isn\u0027t\", \"is not\")\n",
"        (\"it\u0027d\",  \"it would\")\n",
"        (\"kinda\", \"kind of\")\n",
"        (\"let\u0027s\", \"let us\")\n",
"        (\"ma\u0027am\", \"madam\")\n",
"        (\"mayn\u0027t\", \"may not\")\n",
"        (\"may\u0027ve\", \"may have\")\n",
"        (\"methinks\", \"I think\")\n",
"        (\"mightn\u0027t\", \"might not\")\n",
"        (\"might\u0027ve\", \"might have\")\n",
"        (\"mustn\u0027t\", \"must not\")\n",
"        (\"mustn\u0027t\u0027ve\", \"must not have\")\n",
"        (\"must\u0027ve\", \"must have\")\n",
"        (\"needn\u0027t\", \"need not\")\n",
"        (\"shan\u0027t\", \"shall not\")\n",
"        (\"should\u0027ve\", \"should have\")\n",
"        (\"shouldn\u0027t\", \"should not\")\n",
"        (\"shouldn\u0027t\u0027ve\", \"should not have\")\n",
"        (\"that\u0027re\", \"that are\")\n",
"        (\"there\u0027re\", \"there are\")\n",
"        (\"these\u0027re\", \"these are\")\n",
"        (\"these\u0027ve\", \"these have\")\n",
"        (\"they\u0027ll\", \"they will\")\n",
"        (\"they\u0027ve\", \"they have\")\n",
"        (\"they\u0027re\", \"they are\")\n",
"        (\"those\u0027re\", \"those are\")\n",
"        (\"those\u0027ve\", \"those have\")\n",
"        (\"wanna\", \"want to\")\n",
"        (\"wasn\u0027t\", \"was not\")\n",
"        (\"we\u0027d\u0027ve\", \"we would have\")\n",
"        (\"we\u0027ll\", \"we will\")\n",
"        (\"we\u0027re\", \"we are\")\n",
"        (\"we\u0027ve\", \"we have\")\n",
"        (\"weren\u0027t\", \"were not\")\n",
"        (\"what\u0027d\", \"what did\")\n",
"        (\"what\u0027ve\", \"what have\")\n",
"        (\"where\u0027d\", \"where did\")\n",
"        (\"where\u0027re\", \"where are\")\n",
"        (\"where\u0027ve\", \"where have\")\n",
"        (\"which\u0027re\", \"which are\")\n",
"        (\"which\u0027ve\", \"which have\")\n",
"        (\"who\u0027d\u0027ve\", \"who would have\")\n",
"        (\"who\u0027re\", \"who are\")\n",
"        (\"who\u0027s\", \"who has\")\n",
"        (\"who\u0027ve\", \"who have\")\n",
"        (\"why\u0027d\", \"why did\")\n",
"        (\"why\u0027re\", \"why are\")\n",
"        (\"won\u0027t\", \"will not\")\n",
"        (\"would\u0027ve\", \"would have\")\n",
"        (\"wouldn\u0027t\", \"would not\")\n",
"        (\"wouldn\u0027t\u0027ve\", \"would not have\")\n",
"        (\"you\u0027ll\", \"you will\")\n",
"        (\"you\u0027re\", \"you are\")\n",
"        (\"you\u0027ve\", \"you have\")\n",
"        ] |\u003e Map\n",
"\n",
"    /// Tryfind contraction and expand\n",
"    let expand (word: string): option\u003cstring\u003e=\n",
"        if word.Contains(\"\u0027\") then \n",
"            match englishContractions.TryFind word with\n",
"            | Some expandedWord -\u003e Some expandedWord\n",
"            | None -\u003e None\n",
"        else Some word\n",
"\n",
"    let expandContractions (textItem: string) = \n",
"        textItem.Split(\" \")\n",
"        |\u003e Array.choose expand\n",
"        |\u003e String.concat(\" \")\n",
"\n",
"module Tokenization =\n",
"\n",
"    /// NGrams Tokenizer \n",
"    let nGrams (n: int) (text: string) = \n",
"        text.Split(\" \")\n",
"        |\u003e Array.windowed n\n",
"        |\u003e Array.map (String.concat(\" \"))\n",
"\n",
"module NltkData = \n",
"\n",
"    let stopWords = \n",
"        Set [\n",
"        \"i\"\n",
"        \"me\"\n",
"        \"my\"\n",
"        \"myself\"\n",
"        \"we\"\n",
"        \"our\"\n",
"        \"ours\"\n",
"        \"ourselves\"\n",
"        \"you\"\n",
"        \"you\u0027re\"\n",
"        \"you\u0027ve\"\n",
"        \"you\u0027ll\"\n",
"        \"you\u0027d\"\n",
"        \"your\"\n",
"        \"yours\"\n",
"        \"yourself\"\n",
"        \"yourselves\"\n",
"        \"he\"\n",
"        \"him\"\n",
"        \"his\"\n",
"        \"himself\"\n",
"        \"she\"\n",
"        \"she\u0027s\"\n",
"        \"her\"\n",
"        \"hers\"\n",
"        \"herself\"\n",
"        \"it\"\n",
"        \"it\u0027s\"\n",
"        \"its\"\n",
"        \"itself\"\n",
"        \"they\"\n",
"        \"them\"\n",
"        \"their\"\n",
"        \"theirs\"\n",
"        \"themselves\"\n",
"        \"what\"\n",
"        \"which\"\n",
"        \"who\"\n",
"        \"whom\"\n",
"        \"this\"\n",
"        \"that\"\n",
"        \"that\u0027ll\"\n",
"        \"these\"\n",
"        \"those\"\n",
"        \"am\"\n",
"        \"is\"\n",
"        \"are\"\n",
"        \"was\"\n",
"        \"were\"\n",
"        \"be\"\n",
"        \"been\"\n",
"        \"being\"\n",
"        \"have\"\n",
"        \"has\"\n",
"        \"had\"\n",
"        \"having\"\n",
"        \"do\"\n",
"        \"does\"\n",
"        \"did\"\n",
"        \"doing\"\n",
"        \"a\"\n",
"        \"an\"\n",
"        \"the\"\n",
"        \"and\"\n",
"        \"but\"\n",
"        \"if\"\n",
"        \"or\"\n",
"        \"because\"\n",
"        \"as\"\n",
"        \"until\"\n",
"        \"while\"\n",
"        \"of\"\n",
"        \"at\"\n",
"        \"by\"\n",
"        \"for\"\n",
"        \"with\"\n",
"        \"about\"\n",
"        \"against\"\n",
"        \"between\"\n",
"        \"into\"\n",
"        \"through\"\n",
"        \"during\"\n",
"        \"before\"\n",
"        \"after\"\n",
"        \"above\"\n",
"        \"below\"\n",
"        \"to\"\n",
"        \"from\"\n",
"        \"up\"\n",
"        \"down\"\n",
"        \"in\"\n",
"        \"out\"\n",
"        \"on\"\n",
"        \"off\"\n",
"        \"over\"\n",
"        \"under\"\n",
"        \"again\"\n",
"        \"further\"\n",
"        \"then\"\n",
"        \"once\"\n",
"        \"here\"\n",
"        \"there\"\n",
"        \"when\"\n",
"        \"where\"\n",
"        \"why\"\n",
"        \"how\"\n",
"        \"all\"\n",
"        \"any\"\n",
"        \"both\"\n",
"        \"each\"\n",
"        \"few\"\n",
"        \"more\"\n",
"        \"most\"\n",
"        \"other\"\n",
"        \"some\"\n",
"        \"such\"\n",
"        \"no\"\n",
"        \"nor\"\n",
"        \"not\"\n",
"        \"only\"\n",
"        \"own\"\n",
"        \"same\"\n",
"        \"so\"\n",
"        \"than\"\n",
"        \"too\"\n",
"        \"very\"\n",
"        \"can\"\n",
"        \"will\"\n",
"        \"just\"\n",
"        \"don\"\n",
"        \"don\u0027t\"\n",
"        \"should\"\n",
"        \"should\u0027ve\"\n",
"        \"now\"\n",
"        \"ain\"\n",
"        \"aren\"\n",
"        \"aren\u0027t\"\n",
"        \"couldn\"\n",
"        \"couldn\u0027t\"\n",
"        \"didn\"\n",
"        \"didn\u0027t\"\n",
"        \"doesn\"\n",
"        \"doesn\u0027t\"\n",
"        \"hadn\"\n",
"        \"hadn\u0027t\"\n",
"        \"hasn\"\n",
"        \"hasn\u0027t\"\n",
"        \"haven\"\n",
"        \"haven\u0027t\"\n",
"        \"isn\"\n",
"        \"isn\u0027t\"\n",
"        \"ma\"\n",
"        \"mightn\"\n",
"        \"mightn\u0027t\"\n",
"        \"mustn\"\n",
"        \"mustn\u0027t\"\n",
"        \"needn\"\n",
"        \"needn\u0027t\"\n",
"        \"shan\"\n",
"        \"shan\u0027t\"\n",
"        \"shouldn\"\n",
"        \"shouldn\u0027t\"\n",
"        \"wasn\"\n",
"        \"wasn\u0027t\"\n",
"        \"weren\"\n",
"        \"weren\u0027t\"\n",
"        \"won\"\n",
"        \"won\u0027t\"\n",
"        \"wouldn\"\n",
"        \"wouldn\u0027t\"\n",
"        ]\n",
"\n",
"    let removeStopWords (textItem: string) = \n",
"\n",
"        let remaining = \n",
"            textItem.Split(\" \")\n",
"            |\u003e Array.filter (fun word -\u003e not (stopWords.Contains word))\n",
"        if Array.isEmpty remaining then None else Some (remaining |\u003e String.concat(\" \"))\n",
"\n",
"module TermFrequencies = \n",
"\n",
"    let tf bow = \n",
"        \n",
"        let docTokenCounts = \n",
"            Seq.sumBy snd bow\n",
"\n",
"        bow\n",
"        |\u003e Array.map (fun (token, count) -\u003e \n",
"            let tf = (float count)/(float docTokenCounts)\n",
"            token, tf)\n",
"        |\u003e Array.sortByDescending snd\n",
"     \n",
"    let idf bows= \n",
"\n",
"        let numDocs = Seq.length bows\n",
"\n",
"        bows\n",
"        |\u003e Seq.collect (Seq.map fst)\n",
"        |\u003e Seq.countBy id\n",
"        |\u003e Seq.map (fun (token, numDocsWithToken) -\u003e \n",
"            let idf = (float numDocs) / (float numDocsWithToken)\n",
"            token, log idf)\n",
"        |\u003e Seq.sortByDescending snd\n",
"        |\u003e Seq.toArray\n",
"\n",
"    let tfIdf (idf : Map\u003c\u0027Token, float\u003e) \n",
"              bow = \n",
"        \n",
"        let idfPrior = \n",
"            idf \n",
"            |\u003e Map.toArray \n",
"            |\u003e Array.averageBy snd\n",
"\n",
"        tf bow\n",
"        |\u003e Array.choose (fun (token, tf) -\u003e \n",
"            match idf.TryFind token with\n",
"            // Word appeared in train\n",
"            | Some idf -\u003e \n",
"                let tfIdf = tf * idf\n",
"                Some (token, tfIdf)\n",
"            // Word did not appear in train \n",
"            | None -\u003e \n",
"                let tfIdf = tf * idfPrior\n",
"                Some (\"UNK\", tfIdf))\n"]
          }],
            "metadata": {
            "kernelspec": {"display_name": ".NET (F#)", "language": "F#", "name": ".net-fsharp"},
            "langauge_info": {
        "file_extension": ".fs",
        "mimetype": "text/x-fsharp",
        "name": "C#",
        "pygments_lexer": "fsharp",
        "version": "4.5"
        }
        },
            "nbformat": 4,
            "nbformat_minor": 1
        }
        

