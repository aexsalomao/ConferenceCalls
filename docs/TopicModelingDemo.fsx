(**
## Import packages and load scripts
*)

#r "nuget: Plotly.NET, 2.0.0-preview.6"
#r "nuget: FSharp.Collections.ParallelSeq"
#r "nuget: FSharp.Stats"
#r "nuget: FSharp.Data"
#r "nuget: Newtonsoft.Json"

open System
open System.Text.RegularExpressions
open FSharp.Collections.ParallelSeq

Environment.CurrentDirectory <- __SOURCE_DIRECTORY__
fsi.AddPrinter<DateTime>(fun dt -> dt.ToString("s"))

open FSharp.Data
open Newtonsoft.Json
open Plotly.NET
open FSharp.Stats

(**
## Read transcripts from json file
*)

type Label = 
    | Positive
    | Negative
    | Neutral

type LabeledTranscript = 
    { TickerExchange: (string * string) 
      EarningsCall: string
      CumulativeReturn: float 
      Label: Label }

let readJson (jsonFile: string) =
    IO.File.ReadAllText(jsonFile)
    |> fun json -> JsonConvert.DeserializeObject<array<LabeledTranscript>>(json)

let fullSample = 
    readJson ("data-cache/LabeledTranscriptsFullSample.json")

let randSample =
    let rnd = System.Random()
    fullSample
    |> Seq.sortBy (fun _ -> rnd.Next())
    |> Seq.take 500
    |> Seq.toArray

(**
## Topic modeling
*)

(**
Topic detection or **topic modeling** is a technique of automatically extracting meaning from texts
by identifying recurrent themes or topics.

Topic modeling is a method for analyzing large volumes of unlabeld text data. It helps in:
- Discovering hidden topical patterns that are present across the collection

- Annotating documents according to these topics

- Using these annotations to organize, search and summarize texts

A *topic* consists of a cluster of words that frequently occur together.
This is essentially a clustering problem - we can think of both words and documents as being clustered.

There are many techniques that are used to obtain topic models. One of the commonly used is 
**Latent Dirichlet Allocation (LDA)**
*)

(**
### Latent Dirichlet Allocation
*)

(**
LDA is a *probabililstic* model with a corresponding *generative* process,
where each document is assumed to be generated by this (simple) process

**Key assumptions**:

- The distributional hypothesis: words that appear together frequently 
are likely to be close in meaning.

- Each topic is a mixture of different words.

- Each document is a mixture of different topics (but typically,
not many, and typically one is dominant)

- **Number of topics is known in advance**
*)

(**
## Predicting Returns with Text Data
*)

(**
**SESTM: A Supervised Sentiment Extraction Algorithm**

Methodology:

1. Feature selection: create a set of sentiment-charged words via *predictive (correlation) screening*
2. Assign prediction/sentiment weights to these words via a supervised topic model(i.e. estimate positive and negative sentiment topics)
3. Aggregate terms into an article-level predictive score via penalized likelihood.

- Model is motivated by the view that return-predictive content of a given event is 
reflected *both* in the news article text and in the returns of related assets.

- Method has an objective of extracting general *return predictive* content from text.
*)

(**
**Advantages**

- Simplicity: only requires standard econometric techniques such as correlation analysis and maximum likelihood estimation. 
Additionally, unlike other deep learning approaches, the proposed *supervised* learning approach is entirely "white-box".

- Minimal computing power required.

- Free of any pre-existing sentiment dictionary (polarized words, sentiment lexicons, etc...). No use of ad hoc word-weighting schemes.

Bottom line: A sentiment scoring model is constructed from the *joint* behaviour of 
article text and stock returns.
*)

(**
**Theoretical Reusults**

- The guarantee of retrieving a sentiment dictionary from training data via correlation screening.

- The derivation of sharp error bounds for parameter estimation. The error bounds depend on the scale
of the corpus (e.g., size of the vocabulary, total number of text documents, average number of words 
per document, etc.), and the strength of sentiment signals (e.g., the sentivity of returns to sentiment, 
sensitivity of text generation to sentiment, etc.).

- The error of predicting the sentiment score of a newly arriving article is both derived and quantified.
*)

(**
## 1. Screening for Sentiment-Charged words
*)

(**
Objective: Isolate the subset of sentiment-charged words (remove sentiment-neutral words, i.e. noise).

Intuitively, if a word frequently co-occurs in articles that are accompanied
by positive returns, that word is likely to convey positive sentiment.

Methodology:

1. Calculate the frequency with which each word (or phrase) *j* co-occurs with a positive
return.

- $f_{i} = \frac{count of word j in articles with sgn(y) = +1}{k_{j}}$
- $k_{j}$: count of word *j* in all articles

2. Compare $f_{j}$ with proper thresholds.
*)

(**
### DocTransformer: N-Grams
*)

let nGrams (n: int) (text: string): string [] = 
    let onlyWords = Regex(@"(?<!\S)[a-zA-Z0-9]\S*[a-zA-Z0-9](?!\S)")
    
    let findWords words =

        let isWord word = 
            let candidateMatch = onlyWords.Match word
            if candidateMatch.Success then Some candidateMatch 
            else None

        words
        |> Seq.choose isWord
        |> Seq.map (fun m -> m.Value)

    let tryNGram words = 
        if (words |> Seq.length = n) then Some (words |> String.concat(" "))
        else None

    text.Split(" ")
    |> Seq.windowed n
    |> Seq.map findWords
    |> Seq.choose tryNGram
    |> Seq.toArray

(**
### Screening Score ($f_{j}$)

- $f_{i} = \frac{count of word j in articles with sgn(y) = +1}{k_{j}}$
- $k_{j}$: count of word *j* in all articles
*)

type Sign = 
    | Positive
    | Negative

type WordFrequency = 
    { Word : string
      Frequency : float
      SentimentFlag : Sign}

let vocabulary = 
    randSample
    |> Seq.collect (fun xs -> xs.EarningsCall |> nGrams 1)
    |> Seq.distinct
    |> Set

let countWordFrequency (text: string []) (ret: float) (word: string) = 
    let frequency = 
        text
        |> Seq.sumBy (fun w -> 
            if w = word then 1. 
            else 0.)

    let sign = 
        if ret > 0. then Positive 
        else Negative

    if frequency > 0. 
    then Some { Word = word; Frequency = frequency; SentimentFlag = sign }
    else None

let getWordFrequency lt= 
    vocabulary
    |> Seq.choose (fun word -> 
        let text = lt.EarningsCall |> nGrams 1
        let ret = lt.CumulativeReturn
        countWordFrequency text ret word)
    |> Seq.toArray

let sumWordFrequencies wordFrequencies = 
    wordFrequencies
    |> Seq.groupBy (fun xs -> xs.Word, xs.SentimentFlag)
    |> Seq.map (fun (wordSentiment, wordFreq) -> 
        let word, sentiment = wordSentiment
        let freqSum = wordFreq |> Seq.sumBy (fun xs -> xs.Frequency)
        word, { Word = word
                Frequency = freqSum
                SentimentFlag = sentiment })
    |> Seq.toArray

randSample
|> Array.take 5
|> Array.collect getWordFrequency
|> sumWordFrequencies
|> Map

(**
## 2. Learning Sentiment Topics
*)

(**
## 3. Scoring New Articles
*)